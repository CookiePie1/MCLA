\documentclass{article}
\title{Linear Algebra Concise Review} % title - chapter number
\author{John Yang}
\usepackage{amsmath}
\usepackage[margin=0.775in, letterpaper]{geometry}
\usepackage{outlines}
\setcounter{section}{+0} % chapter number minus 1
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{stackrel}
\DeclarePairedDelimiter\set\{\}
\usepackage{hyperref}
\usepackage{mathrsfs}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}
\usepackage{tocloft}
\renewcommand\cftsecfont{\normalfont}
\renewcommand\cftsecpagefont{\normalfont}
\renewcommand{\cftsecleader}{\cftdotfill{\cftsecdotsep}}
\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}

\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\rhead{John Yang}
\lhead{LA Concise Review}
\rfoot{Pg. \thepage}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    \section{Vectors} % chapter title
        \subsection{The Geometry and Algebra of Vectors}
        \begin{outline}
            \1 A vector is a directed line segment that corresponds to a displacement from one point A to another point B. 
            \1 Column vectors and row vectors are different ways to express the same thing: \[[3,2]=\begin{bmatrix}3 \\ 2\end{bmatrix}\]
            \1 The point is that components of vectors are ordered. 
            \1 Two vectors are equal if they have the same magnitude and direction. Two vectors can still be equal if they have different initial and terminal points. 
            \1 Standard position of a vector - when the initial point is at the origin. 
            \1 Sum \(\mathbf u+\mathbf v=[u_1+v_1,u_2+v_2]\)
            \1 Place vectors from head to tail. 
            \1 Scalar multiples: \(c\mathbf v=[cv_1,cv_2]\) aka scaling a vector
            \1 Subtraction is just adding the negative. 
            \1 Properties of vectors in \(\mathbb{R}^n\): let \(\mathbf u\), \(\mathbf v\), and \(\mathbf w\) be vectors in \(\mathbb{R}^n\) and let $c$ and $d$ be scalars. Then: 
                \2 \(\mathbf u+\mathbf v=\mathbf v+\mathbf u\)
                \2 \((\mathbf u+\mathbf v)+\mathbf w=\mathbf u+(\mathbf v+\mathbf w)\)
                \2 \(\mathbf u+\mathbf 0=\mathbf u\)
                \2 \(\mathbf u+(-\mathbf u)=0\)
                \2 \(c(\mathbf u+\mathbf v)=c\mathbf u+c\mathbf v\)
                \2 \((c+d)\mathbf u=c\mathbf u+d\mathbf u\)
                \2 \(c(d\mathbf u)=(cd)\mathbf u\)
                \2 \(1\mathbf u=\mathbf u\)
            \1 A vector $\mathbf v$ is a linear combination of vectors \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_k\) if there are scalars \(c_1,c_2,\cdots,c_k\) such that \(\mathbf v=c_1\mathbf v_1+c_2\mathbf v_2+\cdots+c_k\mathbf v_k\). Those scalars are called the coefficients of the linear combination. 
            \1 Binary vectors - the components are either 0 or 1. 
            \1 Modulus function - divide by a given number and you're left with the remainder. 

        \end{outline}
        \subsection{Length and Angle: the Dot Product}
        \begin{outline}
            \1 dot product: If \[\mathbf u=\begin{bmatrix}u_1\\u_2\\\vdots\\u_n\end{bmatrix}\quad\text{and}\quad\mathbf v=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}\] then the dot product of \(\mathbf u\cdot\mathbf v\) of \(\mathbf u\) and \(\mathbf v\) is defined by \[\mathbf u\cdot\mathbf v=u_1v_1+u_2v_2+\cdots+u_nv_n\]
            \1 properties of dot product: let \(\mathbf u\), \(\mathbf v\), and \(\mathbf w\) be vectors in \(\mathbb R^n\) and let $c$ be a scalar. Then: 
                \2 \(\mathbf{u\cdot v}=\mathbf{v\cdot u}\)
                \2 \(\mathbf{u\cdot}(\mathbf{v+w})=\mathbf{u\cdot v+u\cdot w}\)
                \2 \((c\mathbf u)\mathbf{\cdot v}=c(\mathbf{u\cdot v})\)
                \2 \(\mathbf{u\cdot u\geq 0}\) and \(\mathbf{u\cdot u}=0\) IFF \(\mathbf u=\mathbf 0\)
                \2 Length or norm of a vector \(\mathbf v=\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}\) in \(\mathbb R^n\) is the nonnegative scalar \(\|\mathbf v\|\) defined by \[\|\mathbf v\|=\sqrt{\mathbf{v\cdot v}}=\sqrt{v_1^2+v_2^2+\cdots+v_n^2}\]
            \1 Normalizing a vector means finding the unit vector. 
            \1 Cauchy-Schwarz Inequality: For all vectors \(\mathbf u\) and \(\mathbf v\) in \(\mathbb R^n\), \[|\mathbf{u\cdot v}|\leq \|\mathbf u\|\|\mathbf v\|\]
            \1 Triangle inequality: for all vectors \(\mathbf u\) and \(\mathbf v\) and \(\mathbb R^n\), \[\|\mathbf u+\mathbf v\|\leq\|\mathbf u\|+\|\mathbf v\|\]
            \1 Distance between two vectors is defined by \[d(\mathbf u,\mathbf v)=\|\mathbf u-\mathbf v\|\]
            \1 Two vectors are orthogonal if \(\mathbf u\cdot\mathbf v=0\)
            \1 For all vectors \(\mathbf u\) and \(\mathbf v\) in \(\mathbb R^n\), \(\|\mathbf u+\mathbf v\|^2=\|\mathbf u\|^2+\|\mathbf v\|^2\) IFF \(\mathbf u\) and \(\mathbf v\) are orthogonal. 
            \1 If \(\mathbf u\) and \(\mathbf v\) are vectors in \(\mathbb R^n\) and \(\mathbf u\neq\mathbf 0\), then the projection of \(\mathbf v\) onto \(\mathbf u\) is the vector defined by \[\text{proj}_{\mathbf u}(\mathbf v)=\left(\dfrac{\mathbf{u\cdot v}}{\mathbf{u\cdot u}}\right)\mathbf u\]

        \end{outline}
        \subsection{Lines and Planes}
        \begin{outline}
            \1 Normal form of the equation of a 2D line: \[\mathbf{n\cdot(x-p)=0}\qquad\text{or}\qquad\mathbf{n\cdot x=n\cdot p}\] where \(\mathbf p\) is a specific point on the line and \(\mathbf{n\neq 0}\) is a normal vector for the line. 
            \1 The general form of the equation of the line is \(ax+by=c\) where \(\mathbf n=\begin{bmatrix}a\\b\end{bmatrix}\) is a normal vector for the line. 
            \1 The vector form of the equation of a 2D or 3D line is \[\mathbf{x=p}+t\mathbf{d}\] where \(\mathbf p\) is a specific point on the line and \(\mathbf{d\neq 0}\) is a direction vector for the line. The equations corresponding to the components of the vector form of the equations are called parametric equations of the line. 
            \1 Normal form of the equation of a plane \(\mathscr{P}\) in \(\mathbb R^3\) is \[\mathbf{n\cdot(x-p)}=0\qquad\text{or}\qquad\mathbf{n\cdot x=n\cdot p}\] where \(\mathbf p\) is a specific point on \(\mathscr P\) and \(\mathbf{n\neq 0}\) is a normal vector for \(\mathscr P\). 
            \1 The general form of the equation of \(\mathscr P\) is \(ax+by+cz=d\), where \(\mathbf n=\begin{bmatrix}a\\b\\c\end{bmatrix}\) is a normal vector for \(\mathscr P\). 
            \1 The vector form of the equation of a plane \(\mathscr P\) in \(\mathbb R^3\) is \[\mathbf{x=p}+s\mathbf u+t\mathbf v\] where \(\mathbf p\) is a point on \(\mathscr P\) and \(\mathbf u\) and \(\mathbf v\) are direction vectors for \(\mathscr P\) (\(\mathbf u\) and \(\mathbf v\) are nonzero and parallel to \(\mathscr P\), but not parallel to each other). The equations corresponding to the components of the vector form of the equation are called parametric equations of \(\mathscr P\). 
            \1 Summary of equations of 2D lines:
                \2 Normal form: \(\mathbf{n\cdot x=n\cdot p}\)
                \2 General form: \(ax+by=c\)
                \2 Vector form: \(\mathbf{x=p}+t\mathbf{d}\)
                \2 Parametric form: \[\begin{cases}x=p_1+td_1\\y=p_2+td_2\end{cases}\]
            \1 Summary of equations of 3D lines:
                \2 Normal form: \[\begin{cases}
                    \mathbf{n_1\cdot x=n_1\cdot p_1} \\
                    \mathbf{n_2\cdot x=n_2\cdot p_2}
                \end{cases}\]
                \2 General form: \[\begin{cases}
                    a_1x+b_1y+c_1z=d_1\\
                    a_2x+b_2y+c_2z=d_2
                \end{cases}\]
                \2 Vector form: \(\mathbf{x=p}+t\mathbf d\)
                \2 Parametric form: \[\begin{cases}
                    x=p_1+td_1\\
                    y=p_2+td_2\\
                    z=p_3+td_3
                \end{cases}\]
            \1 Summary of equations of 3D planes: 
                \2 Normal form: \(\mathbf{n\cdot x=n\cdot p}\)
                \2 General form: \(ax+by+cz=d\)
                \2 Vector form: \(\mathbf{x=p}+s\mathbf u+t\mathbf v\)
                \2 Parametric form: \[\begin{cases}
                    x=p_1+su_1+tv_1\\
                    y=p_2+su_2+tv_2\\
                    z=p_3+su_3+tv_3
                \end{cases}\]
        \end{outline}
        \subsection{Applications}
        \begin{outline}
            \1 Force vectors: if the resultant net force is zero, the system is in equilibrium. 
            \1 Resolve into components to work with the vectors. 
        \end{outline}
    \section{Systems of Linear Equations} % chapter title
        \subsection{Introduction to Systems of Linear Equations}
        \begin{outline}
            \1 A linear equation in the $n$ variables \(x_1,x_2,x_3,\cdots,x_n\) is an equation that can be written in the form \[a_1x_1+a_2x_2+\cdots+a_nx_n=b\] where the coefficients \(a_1,a_2,\cdots,a_n\) and the constant term $b$ are constants. 
            \1 A solution of a linear equation \(a_1x_1+a_2x_2+\cdots+a_nx_n=b\) is a vector \([s_1,s_2,\cdots,s_n]\) whose components satisfy the equation when we substitute \(x_1=s_1,x_2=s_2,\cdots,x_n=s_n\). 
            \1 A system of linear equations is a finite set of linear equations, each with the same variables. A solution of a system of linear equations is a vector that is simultaneously a solution of each equation in the system. The solution set of a system of linear equations is the set of all solutions of the system. 
            \1 A system of linear equations is called consistent if it has at least one solution. A system with no solutions is inconsistent. 
            \1 Two linear systems are called equivalent if they have the same solution sets. 
            \1 Solving a matrix with a CAS may not always be the best choice. 
        \end{outline}
        \subsection{Direct Methods for Solving Linear Systems}
        \begin{outline}
            \1 The coefficient matrix contains the coefficients of the variables, and the augmented matrix is the coefficient matrix augmented by an extra column containing the constant terms. 
            \1 A matrix is in row echelon form if it satisfies the following properties: 
                \2 Any rows consisting entirely of zeroes are at the bottom
                \2 In each nonzero row, the first nonzero entry (called the leading entry) is in a column to the left of any leading entries below it. 
            \1 Elementary row operations: 
                \2 Interchange two rows
                \2 Multiply a row by a nonzero constant
                \2 Add a multiple of a row to another row 
            \1 Matrices $A$ and $B$ are row equivalent if there is a sequence of elementary row operations that converts $A$ into $B$. 
            \1 Matrices $A$ and $B$ are row equivalent IFF they can be reduced to the same row echelon form. 
            \1 Gaussian elimination: 
                \2 Write the augmented matrix of the system of linear equations. 
                \2 Use elementary row operations to reduce the augmented matrix to row echelon form. 
                \2 Using back substitution, solve the equivalent system that corresponds to the row-reduced matrix. 
            \1 The rank of a matrix is the number of nonzero rows in its row echelon form. 
            \1 The rank theorem: let $A$ be the coefficient matrix of a system of linear equations with $n$ variables. If the system is consistent, then \[\text{number of free variables}=n-\text{rank}(A)\]
            \1 A matrix is in reduced row echelon form if it satisfies the following:
                \2 It is in row echelon form. 
                \2 The leading entry in each nonzero row is a 1 (called a leading 1)
                \2 Each column containing a leading 1 haas zeroes everywhere else. 
            \1 Steps for Gauss-Jordan Elimination: 
                \2 Write the augmented matrix of the system of linear equations. 
                \2 Use elementary row operations to reduce the augmented matrix to RREF
                \2 If the resulting system is consistent, solve for the leading variables in terms of any remaining free variables. 
            \1 A system of linear equations is called homogeneous if the constant term in each equation is zero. 
            \1 Theorem: If \([A|\mathbf 0]\) is a homogeneous system of $m$ linear equations with $n$ variables, where \(m<n\), then the system has infinitely many solutions. 

        \end{outline}
        \subsection{Spanning Sets and Linear Independence}
        \begin{outline}
            \1 Theorem: A system of linear equations with the augmented matrix \([A|\mathbf b]\) is consistent IFF \(\mathbf b\) is a linear combination of the columns of \(A\). 
            \1 Definition: If \(S=\{\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_k\}\) is a set of vectors in \(\mathbb R^n\), then the set of all linear combinations of \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_k\) is called the span of \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_k\) and is denoted by \(\text{span}(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_k)\) or \(\text{span}(S)\). If \(\text{span}(S)=\mathbb R^n\), then $S$ is called a spanning set for \(\mathbb R^n\). 
            \1 Definition: A set of vectors \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_k\) is linearly dependent if there are scalars \(c_1,c_2,\cdots,c_k\), \textit{at least one of which is not zero}, such that \[c_1\mathbf v_1+c_2\mathbf v_2+\cdots+c_k\mathbf v_k=\mathbf 0\] A set of vectors that is not linearly dependent is called linearly independent. 
            \1 Theorem: Vectors \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_m\) in \(\mathbb R^n\) are linearly dependent IFF at least one of the vectors can be expressed as a linear combination of the others. 
            \1 Theorem: Let \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_m\) be (column) vectors in \(\mathbb{R}^n\) and let $A$ be the \(n\times m\) matrix \([\mathbf\:v_1\mathbf v_2\:\cdots\:\mathbf v_m]\) with these vectors as its columns. Then \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_m\) are linearly dependent IFF the homogeneous linear system with augmented matrix \([A|\mathbf o]\) has a nontrivial solution. 
            \1 Theorem: Let \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_m\) be (row) vectors in \(\mathbb R^n\) and let \(m\times n\) matrix $\begin{bmatrix}
                \mathbf v_1\\\mathbf v_2\\\vdots\\\mathbf v_m
            \end{bmatrix}$ with these vectors is its rows. Then \(\mathbf v_1,\mathbf v_2,\cdots,\mathbf v_m\) are linearly dependent IFF \(\text{rank}(A)<m\). 
            \1 Theorem: Any set of $m$ vectors in \(\mathbb R^n\) is linearly dependent if \(m>n\). 

        \end{outline}
        \subsection{Applications}
        \begin{outline}
            \1 Applications include: 
                \2 Allocation of resources
                \2 Balancing chemical equations 
                \2 Network analysis in transportation, economics, electricity and magnetism
        \end{outline}
        \subsection{Iterative Methods for Solving Linear Systems}
        \begin{outline}
            \1 Two iterative methods: Jacobi's method and Gauss-Seidel method 
            \1 Theorem: If a set of $n$ linear equations in $n$ variables has a strictly diagonally dominant coefficient matrix, then it has a unique solution and both the Jacobi and Gauss-Seidel method converge to it. 
            \1 Theorem: If the Jacobi or the Gauss-Seidel method converges for a system of $n$ linear equations in $n$ varibales, then it must converge to the solution of the system. 

        \end{outline}
    \section{Matrices} % chapter title
        \subsection{Matrix Operations} % section topic
        \begin{outline}
            \1 A matrix is defined as a rectangular array of numbers called the entries, or elements, of the matrix 
            \1 The size of a matrix is based on the number of rows and columns; a matrix with $m$ rows and $n$ columns is an $m \times n$ matrix ($m$ by $n$). 
            \1 Entries of matrix are refered to with double subscripts
            \1 If $m=n$, the matrix is a square matrix. If all nondiagonal entries are $0$, the matrix is a diagonal matrix. A diagonal matrix whose diagonal entries are the same is called a scalar matrix. If the scalar on the diagonal is $1$ it is an identity matrix. 
            \1 Adding matrices: only matrices with the same dimensions can be added. Add each corresponding entry. 
            \1 A matrix whose entries are all 0 is a zero matrix denoted by $O$. 
            \1 Multiplying matrices: If $A$ is an \(m\times n\) matrix and $B$ is an \(n\times r\) matrix, then the product \(C=AB\) is an \(m\times r\) matrix. The \((i,j)\) entry of the product is computed as follows: \[c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{in}b_{nj}\]
            \1 Note that if $A$ is an m by n matrix and B is an n by r matrix, AB is an m by r matrix and n must be equal to n. 
            \1 The product of two matrices is a dot product. 
            \1 Theorem: let $A$ be an \(m\times n\) matrix, \(\vb e_i\) a \(1\times m\) standard unit vector, and \(\vb e_j\) an \(n\times 1\) standard unit vector. Then:
                \2 \(\vb e_iA\) is the \(i\)th row of $A$ and 
                \2 \(A\vb e_j\) is the \(j\)th column of $A$. 
            \1 Partitioned matrices: you can divide a matrix into submatrices by partitioning it into blocks. 
            \1 Matrx powers: \[A^k=AA\cdots A\] by $k$ factors
            \1 If $A$ is a square matrix and $r$ and $s$ are nonnegative integers, then \[A^rA^s=A^{r+s}\qquad\qquad(A^r)^s=A^{rs}\]
            \1 The transpose of an \(m\times n\) matrix $A$ is the \(n\times m\) matrix of \(A^T\) obtained by interchanging the rows and columns of $A$. That is, the \(i\)th column of \(A^T\) is the \(i\)th row of $A$ for all $i$
            \1 A square matrix $A$ is defined as symmetric if \(A^T=A\); that is, if $A$ is equal to its own transpose. 
            \1 A square matrix $A$ is symmetric IFF \(A_{ij}=A_{ji}\) for all $i$ and $j$

       \end{outline}
       \subsection{Matrix Algebra}
       \begin{outline}
            \1 Properties of matrix addition and scalar multiplication
                \2 \(A+B=B+A\)
                \2 \((A+B)+C=A+(B+C)\)
                \2 \(A+O=A\)
                \2 \(A+(-A)=O\)
                \2 \(c(A+B)=cA+cB\)
                \2 \((c+d)A=cA+dA\)
                \2 \(c(dA)=(cd)A\)
                \2 \(1A=A\)
            \1 Linear independence applies to matrices as well
            \1 Properties of matrix multiplication 
                \2 \(A(BC)=(AB)C\)
                \2 \(A(B+C)=AB+AC\)
                \2 \((A+B)C=AC+BC\)
                \2 \(k(AB)=(kA)B=A(kB)\)
                \2 \(I_mA=A=AI_n\text{ if }A\text{ is }m\times n\)
            \1 Properties of the transpose
                \2 \((A^T)^T=A\)
                \2 \((A+B)^T=A^T+B^T\)
                \2 \((kA)^T=k(A^T)\)
                \2 \((AB)^T=B^TA^T\)
                \2 \((A^r)^T=(A^T)^r\) for all nonnegative integers $r$
            \1 Transposing a matrix: like flipping it on its side; rows become columns and columns become rows. Order stays the same; left to right, top to bottom
            \1 If \(A\) is a square matrix, then \(A+A^T\) is a symmetric matrix 
            \1 For any matrix $A$, \(AA^T\) and \(A^TA\) are symmetric matrices. 
       \end{outline}
       \subsection{The Inverse of a Matrix}
       \begin{outline}
           \1 If $A$ is an \(n\times n\) matrix, an inverse of $A$ is an \(n\times n\) matrix \(A'\) with the property that \[AA'=I\quad\text{ and }\quad A'A=I\]  where \(I=I_n\), the \(n\times n\) identity matrix. If \(A'\) exists, then \(A\) is called invertible. 
           \1 If $A$ is an invertible matrix, then its inverse is unique. 
           \1 If \(A\) is an invertible \(n\times n\) matrix, then the system of linear equations given by \(A\vb x=\vb b\) has the unique solution \(\vb x=A^{-1}\vb b\) for any \(\vb b\) in \(\mathbb R^n\)
           \1 If \(A=\begin{bmatrix}
               a & b\\ c& d
           \end{bmatrix}\), then \(A\) is invertible if \(ad-bc\neq 0\), in which case \[A^{-1}=\dfrac{1}{ad-bc}\begin{bmatrix}
               d & -b \\ -c & a
           \end{bmatrix}\] If \(ad-bc=0\), then \(A\) is not invertible. 
           \1 The expression \(ad-bc\) is the determinant of $A$, given by \(\det A\)
           \1 Properties of invertible matrices: 
                \2 If \(A\) is an invertible matrix, then \(A^{-1}\) is invertible and \[(A^{-1})^{-1}=A\]
                \2 If $A$ is an invertible matrix and $c$ is a nonzero scalar, then \(cA\) is an invertible matrix and \[(cA)^{-1}=\dfrac{1}{c}A^{-1}\]
                \2 If \(A\) and \(B\) are invertible matrices of the same size, then \(AB\) is invertible, and \[(AB)^{-1}=B^{-1}A^{-1}\]
                \2 If \(A\) is an invertible matrix, then \(A^T\) is invertible and \[(A^T)^{-1}=(A^{-1})^T\]
                \2 If $A$ is an invertible matrix, then \(A^n\) is invertible for all nonnegative integers $n$ and \[(A^n)^{-1}=(A^{-1})^n\]
            \1 The inverse of a product of invertible matrices is the product of their inverses in reverse order. 
            \1 If $A$ is an invertible matrix and \(n\) is a positive integer, then \(A^{-n}\) is defined by \[A^{-n}=(A^{-1})^n=(A^n)^{-1}\]
            \1 An elementary matrix is one that can be obtained by performing an elementary row operation on an identity matrix. 
            \1 Let \(E\)  by the elementary matrix obtained by performing an elementary row operation on \(I_n\). If the same elementary row operation is performed on an \(n\times r\) matrix $A$, the result is the same as the matrix \(EA\). 
            \1 Each elementary matrix is invertible, and its inverse is an elementary matrix of the same type. 
            \1 The fundamental theorem of invertible matrices: version 1. Let $A$ be an \(n\times n\) matrix. The following statements are equivalent: 
                \2 \(A\) is invertible. 
                \2 \(A\vb x=\vb b\) has a unique solution for every \(\vb b\) in \(\mathbb R^n\)
                \2 \(A\vb x=\vb 0\) has only the trivial solution. 
                \2 The RREF of \(A\) is \(I_n\)
                \2 \(A\) is a product of elementary matrices. 
            \1 Let \(A\) be a square matrix. If \(B\) is a square matrix such that either \(AB=I\) or \(BA=1\), then \(A\) is invertible and \(B=A^{-1}\)
            \1 Let \(A\) be a square matrix. If a sequence of elementary row operations reduces \(A\) to \(I\), then the same sequence of elementary row operations transforms \(I\) into \(A^{-1}\)

       \end{outline}
       \subsection{The LU Factorization}
       \begin{outline}
            \1 Let \(A\) be a square matrix. A factorization of \(A\) as \(A=LU\), where \(L\) is unit lower triangular and \(U\) is upper triangular, is called an \(LU\) factorization of \(A\). 
            \1 If \(A\) is a square matrix that can be reduced to REF without using any row interchanges, then \(A\) has an \(LU\) factorization. 
            \1 If \(A\) is an invertible matrix that has an \(LU\) factorization, then \(L\) and \(U\) are unique. 
            \1 If \(P\) is a permutation matrix, then \(P^{-1}=P^T\)
            \1 Let \(A\) be a square matrix. A factorization of \(A\) as \(A=P^TLU\), where \(P\) is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular, is called a \(P^TLU\) factorization of \(A\)
            \1 Every square matrix has a \(P^TLU\) factorization. 
       \end{outline}
       \subsection{Subspaces, Basis, Dimension, and Rank}
       \begin{outline}
            \1 A subspace of \(\mathbb R^n\) is any collection of $S$ vectors in \(\mathbb R^n\) such that 
                \2 The zero vector \(\vb 0\) is in $S$
                \2 If \(\vb u\) and \(\vb v\) are in $S$, then \(\vb u+\vb v\) is in $S$ (That is, $S$ is closed under addition). 
                \2 If \(vb u\) is in $S$ and $c$ is a scalar, then \(c\vb u\) is in $S$ ($S$ is closed under scalar multiplication). 
                \2 From the previous two conditions, we conclude that $S$ must then be closed under linear combinations: $S$ includes all linear combinations of all vectors \(\vb u_k\) in $S$. 
            \1 Let \(\vb v_1,\vb v_2,\cdots,\vb v_k\) be vectors in \(\mathbb R^n\). Then, \(\text{span}(\vb v_1,\vb v_2,\cdots,\vb v_k)\) is a subspace of \(\mathbb R^n\)
            \1 Let \(A\) be an \(m\times n\) matrix. 
                \2 The row space of $A$ is the subspace row\((A)\) of \(\mathbb R^n\) spanned by the rows of \(A\). 
                \2 The column space of \(A\) is the subspace col\((A)\) of \(\mathbb R^m\) spanned by the columns of \(A\). 
            \1 Let \(B\) be any matrix that is row equivalent to a matrix \(A\). Then \(\text{row}(B)=\text{row}(A)\)
            \1 Let \(A\) be an \(m\times n\) matrix and let \(N\) be the set of solutions to the homogeneous linear system \(A\vb x=\vb 0\). Then \(N\) is a subspace of \(\mathbb R^n\)
            \1 Let \(A\) be an \(m\times n\) matrix. The null space of \(A\) is the subspace of \(\mathbb R^n\) consisting of solutions to the homogeneous linear system \(A\vb x=0\). It is denoted by \(\text{null}(A)\)
            \1 Let \(A\) be a matrix whose entries are real numbers. For any system of linear equations \(A\vb x=\vb b\), exactly one of the following is true: 
                \2 There is no solution. 
                \2 There is a unique solution. 
                \2 There are infinitely many solutions. 
            \1 A basis for a subspace $S$ of \(\mathbb R^n\) is a set of vectors in $S$ that 
                \2 spans $S$ and 
                \2 Is linearly independent. 
            \1 How to find the bases for the row space, column space, and null space of matrix \(A\):
                \2 Find the rref $R$ of $A$. 
                \2 Use the nonzero row vectors of $R$ (containing the leading 1s) to form a basis for row\((A)\)
                \2 Use the column vectors of \(A\) that correspond to the columns of $R$ containing the leading 1s (the pivot columns) to form a basis for col\((A)\)
                \2 Solve for the leading variables of \(R\vb x=0\) in terms of the free variables, set the free variables equal to parameters, substitute back into \(\vb x\), and write the result as a linear combination of $f$ vectors (where $f$ is the number of free variables). These $f$ vectors form a basis for null\((A)\)
            \1 The Basis Theorem: Let $S$ be a subspace of \(\mathbb R^n\). Then any two bases for $S$ have the same number of vectors. 
            \1 If $S$ is a subspace of \(\mathbb R^n\), then the number of vectors in a basis for $S$ is called the dimension of $S$, denoted dim $S$. 
            \1 The row and column spaces of a matrix $A$ have the same dimension. 
            \1 The rank of a matrix $A$ is the dimension of its row and column spaces and is denoted by rank\((A)\). 
            \1 For any matrix $A$, \[\text{rank}(A^T)=\text{rank}(A)\]
            \1 The nullity of a matrix $A$ is the dimension of its null space and is denoted by nullity\((A)\). 
            \1 The Rank Theorem: If \(A\) is an \(m\times n\) matrix, then \[\text{rank}(A)+\text{nullity}(A)=n\]
            \1 The fundamental theorem of invertible matrices: version 2. Let \(A\) be an \(m\times n\) matrix. The following statements are equivalent: 
                \2 \(A\) is invertible. 
                \2 \(A\vb x=b\) has a unique solution for every \(\vb b\) in \(\mathbb R^n\)
                \2 \(A\vb x=\vb 0\) has only the trivial solution. 
                \2 The rref of $A$  is \(I_n\)
                \2 \(A\) is a product of elementary matrices. 
                \2 rank\((A)=n\)
                \2 nullity\((A)=0\)
                \2 The column vectors of \(A\) are linearly independent 
                \2 The column vectors of \(A\) span \(\mathbb R^n\). 
                \2 The column vectors of \(A\) form a basis for \(\mathbb R^n\). 
                \2 The row vectors of \(A\) are linearly independent. 
                \2 The row vectors of \(A\) span \(\mathbb R^n\)
                \2 The row vectors of \(A\) form a basis for \(\mathbb R^n\). 
            \1 Let \(A\) be an \(m\times n\) matrix. Then 
                \2 rank\((A^TA)=\text{rank}(A)\)
                \2 The \(n\times n\) matrix \(A^TA\) is invertible IFF rank\((A)=n\)
            \1 Let $S$ be a subspace of \(\mathbb R^n\) and let \(\mathcal B=\{\vb v_1,\vb v_2,\cdots,\vb v_k\}\) be a basis for $S$. For every vector \(\vb v\) in $S$, there is exactly one way to write \(\vb v\) as a linear combination of the basis vectors in \(\mathcal B\): \[\vb v=c_1\vb v_1+c_2\vb v_2+\cdots+c_k\vb v_k\]
            \1 Let $S$ be a subspace of \(\mathbb R^n\) and let \(\mathcal B=\{\vb v_1,\vb v_2,\cdots,\vb v_k\}\) be a basis for $S$. Let \(\vb v\) be a vector in $S$, and write \(\vb v=c_1\vb v_1+c_2\vb v_2+\cdots+c_k\vb v_k\). Then \(c_1,c_2,\cdots,c_k\) are called the coordinates of \(\vb v\) with respect to \(\mathcal B\), and the column vector \[[\vb v]_\mathcal B=\begin{bmatrix}
                c_1\\c_2\\\vdots\\c_k
            \end{bmatrix}\] is called the coordinate vector of \(\vb v\) with respect to \(\mathcal B\). 
       \end{outline}
       \subsection{Introduction to Linear Transformations}
       \begin{outline}
            \1 A transformation \(T:\mathbb R^n\to \mathbb R^m\) is called a linear transformation if:
                \2 \(T(\vb u+\vb v)=T(\vb u)+T(\vb v)\) for all \(u\) and \(v\) in \(\mathbb R^n\) and 
                \2 \(T(c\vb v)=cT(\vb v)\) for all \(\vb v\) in \(\mathbb R^n\) and all scalars $c$. 
            \1 Let \(A\) be an \(m\times n\) matrix. Then the matrix transformation \(T_A:\mathbb R^n\to\mathbb R^m\) defined by \[T_A(\vb x)=A\vb x\quad (\text{for }\vb x\text{ in }\mathbb R^n)\] is a linear transformation. 
            \1 Let \(T:\mathbb R^n\to \mathbb R^m\) be a linear transformation. Then \(T\) is a matrix transformation. More specifically, \(T=T_A\), where \(A\) is the \(m\times n\) matrix \[A=[T(\vb e_1)\vdots T(\vb e_2)\vdots\cdots\vdots T(\vb e_n)]\]
            \1 Let \(T:\mathbb R^m\to \mathbb R^n\) and \(S:\mathbb R^n\to\mathbb R^p\) be linear Transformations. then \(S\circ T:\mathbb R^m\to\mathbb R^p\) is a linear transformation whose standard matrices are related by \[S\circ T=[S][T]\]
            \1 Let $s$ and $T$ be linear transformations from \(\mathbb R^n\) to \(\mathbb R^n\). Then $S$ and $T$ are inverse transformations if \(S\circ T=I_n\) and \(T\circ S=I_n\)
            \1 Let \(T:\mathbb R^n\to\mathbb R^n\) be an invertible linear transformation. Then its standard matrix \([T]\) is an invertible matrix, and \[[T^{-1}]=[T]^{-1}\] (``The matrix of the inverse is the inverse of the matrix''). 

       \end{outline}
       \subsection{Applications}
       \begin{outline}
            \1  Markov chain: evolving proccess consisting of a finite number of states. 
            \1 Can use linear algebra to analyze probabilities. Consider two-way tables in statistics: working with multiple of these tables as matrices and vectors can allow us to solve probability problems. 
            \1 Graphs and digraphs: If $G$ is a graph with $n$ vertices, then its adjacency matrix is the \(n\times n\) matrix $A$ (or \(A(G)\)) defined by \[a_{ij}=1 \text{ if there is an edge between vertices i and j, and }a_{ij}=0\text{ otherwise. }\]
            \1 If $A$ is the adjacency matrix of a graph $G$, then the \((i,j)\) entry of \(A^k\) is equal to the number of \(k\)-paths between vertices $i$ and $j$. 
            \1 If $G$ is a digraph with $n$ vertices, then its adjacency matrix is the \(n\times n\) matrix $A$ (or \(A(G)\)) defined by \[a_{ij}=1 \text{ if there is an edge between vertices i and j, and }a_{ij}=0\text{ otherwise. }\]
            \1 Error correcting codes: If \(k<n\), then any \(n\times k\) matrix of the form \(G=\begin{bmatrix}
                I_k\\A
            \end{bmatrix}\), where $A$ is an \((n-k)\times k\) matrix over \(\mathbb Z_2\), is called a standard generator matrix for an \((n,k)\) binary code \(T:\mathbb Z^k_2\to\mathbb Z^n_2\). Any \((n-k)\times n\) matrix of the form \(P=[B\text{ }I_{n-k}]\), where $B$ is an \((n-k)\times k\) matrix over \(\mathbb Z_2\), is called a stardard parity check matrix. The code is said to have length $n$ and dimension $k$. 
            \1 If \(G=\begin{bmatrix}
                I_k\\A
            \end{bmatrix}\)  is a standard generator matrix and \(P=[B\text{ }I_{n-k}]\) is a standard parity check matrix, then $P$ is the parity check matrix associated with $G$ IFF \(A=B\). The corresponding \(n,k\) binary code is (single) error-correcting IFF the columns of $P$ are nonzero and distinct. 
            \1 Summary of error-correcting codes: 
                \2 For \(n>k\), and \(n\times k\) matrix $G$ and an \((n-k)\times n\) matrix $P$ (with entries in \(mathbb Z_2\)) are a standard generator matrix and a standard parity check matrix, respectively, for an \((n,k)\) binary code IFF in block form, \(G=\begin{bmatrix}
                    I_k\\A
                \end{bmatrix}\) and \(P=[A\text{ }I_{n-k}]\) for some \((n-k)\times k\) matrix $A$ over \(\mathbb Z_2\). 
                \2 $G$ encodes a message vector \(\vb x\) in \(\mathbb Z^k_2\) as a code vector $\vb c$ in \(\mathbb Z^n_2\) via \(\vb c=G\vb x\). 
                \2 $G$ is error-correcting IFF the columns of $P$ are nonzero and distinct. A vector \(\vb c'\) in \(\mathbb Z^n_2\) is a code vector IFF \(P\vb c'=\vb 0\). In this case, the corresponding message vector is the vector \(\vb x\) in \(\mathbb Z^k_2\) consisting of the first $k$ components of \(\vb c'\). If \(P\vb c'\neq 0\), then \(\vb c'\) is not a code vector and \(P\vb c'\) is one of the columns of $P$. If \(P\vb c'\) is the $i$th column of $P$, then the error is in the $i$th component of \(\vb c'\) and we can recover the correct code vector (and hence the message) by changing this component. 


        \end{outline}
    \section{Eigenvalues and Eigenvectors} % chapter title
        \subsection{Introduction to Eigenvalues and Eigenvectors} % section topic
        \begin{outline}
            \1 Let $A$ be an \(n\times n\) matrix. A scalar \(\lambda\) is called an eigenvalue of $A$ if there is a nonzero vector \(\vb x\) such that \(A\vb x=\lambda\vb x\). Such a vector \(\vb x\) is called an eigenvector of $A$ corresponding to \(\lambda\). 
            \1 Let $A$ be an \(n\times n\) matrix and let \(\lambda\) be an eigenvalue of $A$. The collection of all eigenvectors corresponding to \(\lambda\), together with the zero vector, is called the eigenspace of \(\lambda\) and is denoted by \(E_\lambda\). 
        \end{outline}
        \subsection{Determinants}
        \begin{outline}
            \1 Let \(A = \begin{bmatrix}
                a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}
            \end{bmatrix}\). Then the determinant of $A$ is the scalar \[\det A = |A| = a_{11}\begin{vmatrix}
                a_{22} & a_{23} \\ a_{32} & a_{33}
            \end{vmatrix}+a_{13}\begin{vmatrix}
                a_{21} & a_{22} \\ a_{31} & a_{32}
            \end{vmatrix}\]
            \1 We can simplify this equation as: \[\det A=a_{11}\det A_{11} - a_{12}\det A_{12}+a_{13}\det A_{13}\]\[=\sum^3_{j=1}(-1)^{1+j}a_{ij}\det A_{ij}\]
            \1 For any square matrix $A$, \(\det A_{ij}\) is called the \((i,j)\)-minor of $A$. 
            \1 Let \(A=[a_{ij}]\) be an \(n\times n\) matrix, where \(n\geq 2\). Then the determinant of $A$ is the scalar \[\det A=|A|=a_{11}\det A_{11}-a_{12}\det A_{12}+\cdots+(-1)^{1+n}a_{1n}\det A_{1n}\]\[=\sum^n_{j=1}(-1)^{1+j}a_{1j}\det A_{1j}\]
            \1 The \((i,j)\)-cofactor of $A$ is defined as \[C_{ij}=(-1)^{i+j}\det A_{ij}\]
            \1 Thus, the definition of the determinant becomes \[\det A=\sum^n_{j=1}a_{1j}C_{1j}\]
            \1 The Laplace Expansion Theorem: The determinant of an \(n\times n\) matrix \(A=[a_{ij}]\), where \(n\geq 2\), can be computed as \[\det A=a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in}\]\[\sum^n_{j=1}a_{ij}C_{ij}\] (which is the cofactor expansion along the $i$th row) and also as \[\det A=a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}\]\[=\sum^n_{i=1}a_{ij}C_{ij}\] (the cofactor expansion along the $j$th column). 
            \1 The determiniant of a triangular matrix is the product of the entries on its main diagonal. Specifically, if \(A=[a_{ij}]\) is an \(n\times n\) triangular matrix, then \[\det A=a_{11}a_{22}\cdots a_{nn}\]
            \1 Properties of determinants: let \(A\) be a square matrix. 
                \2 If $A$ has a zero row (column), then \(\det A=0\)
                \2 If $B$ is obtained by interchanging two rows (columns) of $A$, then \(\det B=-\det A\)
                \2 If $A$ has two identical rows (columns), then \(\det A=0\)
                \2 If $B$ is obtained by multiplying a row (column) of $A$ by $k$, then \(\det B=k\det A\)
                \2 If $A$, $B$, and $C$ are identical except that the $i$th row (column) of $C$ is the sum of the $i$th rows (columns) of \(A\) and \(B\), then \(\det C=\det A+\det B\)
                \2 If $B$ is obtained by adding a multiple of one row (column) of $A$ to another row (column), then \(\det B=\det A\)
            \1 Let $E$ be an \(n\times n\) elementary matrix. 
                \2 If \(E\) results from interchanging two rows of $I_n$, then \(\det E=-1\)
                \2 If \(E\) results from multiplying one row of \(I_n\) by \(k\), then \(\det E=k\)
                \2 If \(E\) results from adding a multiple of one row of \(I_n\) to another row, then \(\det E=1\)
            \1 Let \(B\) be an \(n\times n\) matrix and let \(E\) be an \(n\times n\) elementary matrix. Then \[\det (EB)=(\det E)(\det B)\]
            \1 A square matrix \(A\) is invertible IFF \(\det A\neq 0\)
            \1 If \(A\) is an \(n\times n\) matrix, then \[\det (kA)=k^n\det A\]
            \1 If $A$ and $B$ are \(n\times n\) matrices, then \[\det (AB)=(\det A)(\det B)\]
            \1 If \(A\) is invertible, then \[\det (A^{-1})=\dfrac{1}{\det A}\]
            \1 For any square matrix $A$, \[\det A=\det A^T\]
            \1 Cramer's rule: let $A$ be an invertible \(n\times n\) matrix and let \(\vb b\) be a vector in \(\mathbb R^n\). Then the unique solution \(\vb x\) of the system \(A\vb x=\vb b\) is given by \[x_i=\dfrac{\det (A_i(\vb b))}{\det A}\qquad\text{for }i=1,\cdots,n\]
            \1 Let \(A\) be an invertible \(n\times n\) matrix. Then \[A^{-1}=\dfrac{1}{\det A}\text{adj }A\] where the adjoint of $A$ adj $A$ is defined by \[[C_{ji}]=[C_{ij}]^T=\begin{bmatrix}
                C_{11} & C_{21} & \cdots & C_{n1} \\ C_{12} & C_{22} & \cdots & C_{n2} \\ \vdots & \vdots & \ddots & \vdots \\ C_{1n} & C_{2n} & \cdots & C_{nn}
            \end{bmatrix}\]
            \1 Let $A$ be an \(n\times n\) matrix. Then \[a_{11}C_{11}+a_{12}C_{12}+\cdots+a_{1n}C_{1n}=\det A=a_{11}C_{11}+A_{21}C_{21}+\cdots+a_{n1}C_{n1}\]
            \1 Let $A$ be an \(n\times n\) matrix and let $B$ be obtained by interchanging any two rows (columns) of $A$. Then \[\det B=-\det A\]
        \end{outline}

        \subsection{Eigenvalues and Eigenvectors of $n\times n$ Matrices}
        \begin{outline}
            \1 The eigenvalues of a square matrix $A$ are preciesely the solutions \(\lambda\) of the equation \[\det (A-\lambda I) = 0\]. 
            \1 Finding the eigenvalues and eigenvectors of a matrix: Let $A$ be an \(n\times n\) matrix. 
                \2 Compute the characteristic polynomial \(\det (A-\lambda I)\) of $A$. 
                \2 Find the eigenvalues of $A$ by solving the characteristic equation \(\det (A-\lambda I)=0\) for \(\lambda\)
                \2 For each eigenvalue \(\lambda\), find the null space of the matrix \(A-\lambda I\). This is the eigenspace \(E_\lambda\), the nonzero vectors of which are the eigenvectors of $A$ corresponding to \(\lambda\). 
                \2 Find a basis for each eigenspace. 
            \1 The algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic equation. An \(n\times n\) matrix will always have $n$ eigenvalues, but some will be duplicates due to algebraic multiplicity. 
            \1 The eigenvalues of a triangular matrix are the entries on its main diagonal. 
            \1 A square matrix $A$ is invertible IFF $0$ is not an eigenvalue of $A$. 
            \1 The fundamental theorem of invertible matrices: version 3. Let $A$ be an $n\times n$ matrix. The following statements are equivalent: 
                \2 $A$ is invertible
                \2 $A\vb x=\vb b$ has a unique solution for every \(\vb b\) in \(\mathbb R^n\)
                \2 \(A\vb x=\vb 0\) has only the trivial solution. 
                \2 The reduced row echelon form of $A$ is \(I_n\). 
                \2 $A$ is the product of elementary matrices. 
                \2 \(\rank (A)=n\)
                \2 nullity\((A)=0\)
                \2 The column vectors of $A$ are linearly independent
                \2 The column vectors of $A$ span \(\mathbb R^n\)
                \2 The column vectors of $A$ form a basis for \(\mathbb R^n\)
                \2 The row vectors of $A$ are linearly independent
                \2 The row vectors of $A$ span \(\mathbb R^n\)
                \2 The row vectors of $A$ form a basis for \(\mathbb R^n\)
                \2 \(\det A\neq 0\)
                \2 $0$ is not an eigenvalue of $A$
            \1 Let $A$ be a square matrix with eigenvalue \(\lambda\) and corresponding eigenvector \(\vb x\). 
                \2 For any positive integer $n$, \(\lambda ^n\) is an eigenvalue of \(A^n\) with corresponding eigenvector \(\vb x\). 
                \2 If $A$ is invertible, then \(1/\lambda\) is an eigenvalue of \(A^{-1}\) with corresponding eigenvector \(\vb x\). 
                \2 If \(A\) is invertible, then for any integer \(n\), \(\lambda^n\) is an eigenvalue of \(A^n\) with corresponding eigenvector \(\vb x\). 
            \1 Suppose the \(n\times n\) matrix $A$ has eigenvectors \(\vb v_1,\vb v_2\cdots,\vb v_m\) with corresponding eigenvalues \(\lambda_1,\lambda_2,\cdots,\lambda_m\). If \(\vb x\) is a vector in \(\mathbb R^n\) that can be expressed as a linear combination of these eigenvectors, then for any integer $k$, \[A^k\vb x=c_1\lambda_1^k\vb v_1+c_2\lambda_2^k\vb v_2+\cdots+c_m\lambda_m^k\vb v_m\]
            \1 Let $A$ be an \(n\times n\) matrix and let \(\lambda_1,\lambda_2,\cdots,\lambda_m\) be distinct eigenvalues of $A$ with corresponding eigenvectors \(\vb v_1,\vb v_2,\cdots,\vb v_m\). Then \(\vb v_1,\vb v_2,\cdots,\vb v_m\) are linearly independent. 

        \end{outline}

        \subsection{Similarity and Diagonalization}
        \begin{outline}
            \1 Let $A$ and $B$ be \(n\times n\) matrices. We say that $A$ is similar to $B$ if there is an invertible \(n\times n\) matrix $P$ such that \(P^{-1}AP=B\). If \(A\) is similar to \(B\), we write \(A\sim B\)
            \1 Let \(A,B,C\) be \(n\times n\) matrices. 
                \2 \(A\sim A\)
                \2 If \(A\sim B\) then \(B\sim A\)
                \2 If \(A\sim B\) and \(B\sim C\) then \(A\sim C\)
            \1 Let \(A\) and \(B\) be \(n\times n\) matrices with \(A\sim B\). Then 
                \2 \(\det A=\det B\)
                \2 \(A\) is invertible IFF \(B\) is invertible. 
                \2 \(A\) and \(B\) have the same rank 
                \2 \(A\) and \(B\) have the same characteristic polynomial. 
                \2 \(A\) and \(B\) have the same eigenvalues. 
                \2 \(A^m\sim B^m\) for all integers \(m\geq 0\)
                \2 If \(A\) is invertible, then \(A^m\sim B^m\) for all integers \(m\). 
            \1 An \(n\times n\) matrix \(A\) is diagonalizable if there is a diagonal matrix \(D\) such that \(A\) is similar to \(D\) - that is, if there is an invertible \(n\times n\) matrix \(P\) such that \(P^{-1}AP=D\)
            \1 Let \(A\) be an \(n\times n\) matrix. Then \(A\) is diagonalizable IFF $A$ has \(n\) linearly independent eigenvectors. More precisely, there exist an invertible matrix $P$ and a diagonal matrix $D$ such that \(P^{-1}AP=D\) IFF the columns of $P$ are \(n\) linearly independent eigenvectors of \(A\) and the diagonal entries of $D$ are the eigenvalues of $A$ corresponding to the eigenvectors in $P$ in the same order. 
            \1 Let \(A\) be an \(n\times n\) matrix and let \(\lambda_1,\lambda_2,\cdots,\lambda_k\) be distinct eigenvalues of $A$. If \(\mathcal B_i\) is a basis for the eigenspace \(E_\lambda\), then \(\mathcal B=\mathcal B_1\cup\mathcal B_2\cup\cdots\cup\mathcal B_k\) (i.e., the total collection of basis vectors for all the eigenspaces) is linearly independent. 
            \1 If $A$ is an \(n\times n\) matrix with \(n\) distinct eigenvalues, then \(A\) is diagonalizable
            \1 If $A$ is an \(n\times n\) matrix, then the geometric multiplicity of each eigenvalue is less than or equal to its algebraic multiplicity. 
            \1 The diagonalization theorem: Let $A$ be an \(n\times n\) matrix whose distinct eigenvalues are \(\lambda_1,\lambda_2,\cdots,\lambda_k\). The following statements are equivalent. 
                \2 $A$ is diagonalizable. 
                \2 The union \(\mathcal B\) of the bases of the eigenspaces of $A$ (as in theorem 4.24) contains \(n\) vectors. 
                \2 The algebraic multiplicity of each eigenvalue equals its geometric multiplicity. 

        \end{outline}

        \subsection{Iterative Methods for Computing Eigenvalues}
        \begin{outline}
            \1 Let \(A\) be an \(n\times n\) diagonalizable matrix with dominant eigenvalue \(\lambda_1\). Then there exists a nonzero vector \(\vb x_0\) such that the sequence of vectors \(\vb x_k\) defined by \[\vb x_1=A\vb x_0,\vb x_2=A\vb x_1,\vb x_3=A\vb x_2,\cdots,\vb x_k=A\vb x_{k-1},\cdots\] approaches a dominant eigenvector of $A$
            \1 Summarization of the power method: Let \(A\) be a diagonalizable \(n\times n\) matrix with a corresponding dominant eigenvalue \(\lambda_1\)
                \2 Let \(\vb x_0=\vb y_0\) be any initial vector in \(\mathbb R^n\) whose largest component is \(1\). 
                \2 Repeat the following steps for \(k=1,2,\cdots\):
                    \3 Compute \(\vb x_k=A\vb y_{k-1}\)
                    \3 Let \(m_k\) be the component of \(\vb x_k\) with the largest absolute value. 
                    \3 Set \(\vb y_k=(1/m_k)\vb x_k\)
            \1 For most choices of \(\vb x_0\), \(m_k\) converges to the dominant eigenvalue \(\lambda_1\) and \(\vb y_k\) converges to a dominant eigenvector. 
            \1 Let \(A=[a_{ij}]\) be a (real or complex) \(n\times n\) matrix, and let \(r_i\) denote the sum of the absolute values of the off-diagonal entries in the $i$th row of $A$; that is, \(r_i=\sum_{j\neq i}|a_{ij}|\). The $i$th Gerschgorin disk is the circular disk $D_i$ in the complex plane with center \(a_{ii}\) and radius \(r_i\). That is, \[D_i=\{z\in\mathbb C:|z-a_{ii}|\leq r_i\}\]
            \1 Gerschgorin's Disk Theorem: Let \(A\) be an \(n\times n\) (real or complex) matrix. Then every eigenvalue of $A$ is contained within a Gerschgorin disk. 
        \end{outline}

        \subsection{Applications and the Perron-Frobenius Theorem}
        \begin{outline}
            \1 If \(P\) is the \(n\times n\) transition matrix of a Markov chain, then 1 is an eigenvalue of $P$. 
            \1 Let \(P\) be an \(n\times n\) transition matrix with eigenvalue \(\lambda\). 
                \2 \(|\lambda|\leq 1\) 
                \2 If $P$ is regular and \(\lambda\neq 1\), then \(|\lambda|<1\)
            \1 Let $P$ be a regular \(n\times n\) transition matrix. If $P$ is diagonalizable, then the dominant eigenvalue \(\lambda_1=1\) has algebraic multiplicity 1
            \1 Let $P$ be a regular \(n\times n\) transition matrix. Then as \(k\to \infty\), \(p^k\) approcahes an \(n\times n\) matrix $L$ whose columns are identical, each equal to the same vector \(\vb x\). This vector \(\vb x\) is a steady state probability vector for $P$. 
            \1 Let $P$ be a regular \(n\times n\) transition matrix, with \(\vb x\) the steady state probability vector for $P$, as in the above. Then, for any initial probability vector \(\vb x_0\), the sequence of iterates \(\vb x_k\) approaches \(\vb x\). 
            \1 Every Leslie matrix has a unique positive eigenvalue and a corresponding eigenvector with positive components. 
            \1 Perron's Theroem: Let $A$ be a positive \(n\times n\) matrix. Then \(A\) has a real eigenvalue \(\lambda_1\) with the following properties: 
                \2 \(\lambda_1>0\)
                \2 \(\lambda_1\) has a corresponding positive eigenvector. 
                \2 If \(\lambda\) is any other eigenvalue of $A$, then \(|\lambda|\leq\lambda_1\)
            \1 The Perron-Frobenius Theorem: Let \(A\) be an irreducible nonnegative \(n\times n\) matrix. Then \(A\) has a real eigenvalue \(\lambda_1\) with the following properties: 
                \2 \(\lambda_1>0\)
                \2 \(\lambda_1\) has a corresponding positive eigenvector. 
                \2 If \(\lambda\) is any other eigenvalue of $A$, then \(|\lambda|\leq\lambda_1\). If $A$ is primitive, then this inequality is strict. 
                \2 If \(\lambda\) is an eigenvalue of $A$ such that \(|\lambda|=\lambda_1\), then \(\lambda\) is a (complex) root of the equation \(\lambda^n-\lambda_1^n=0\)
                \2 \(\lambda_1\) has algebraic multiplicity 1. 
            \1 Def: Let \((x_n)=(x_0,x_1,x_2)\) be a sequence of numbers that is defined as follows: 
                \2 \(x_0=a_0,x_1=a_1,\cdots,x_{k-1}=a_{k-1}\), where \(a_0,a_1,\cdots,a_{k-1}\) are scalars. 
                \2 For all \(n\geq k,x_n=c_1x_{n-1}+c_2x_{n-2}+\cdots+c_kx_{n-k}\), where \(c_1,c_2,\cdots,c_k\) are scalars. 
            \1 If \(c_k\neq 0\), the equation in the second line is called a linear recurrence relation of order $k$. The equations in the first line are referred to as the initial conditions of the recurrence. 
            \1 Let \(x_n=ax_{n-1}+bx_{n-2}\) be a recurrence relation that is satisfied by a sequence \((x_n)\). Let \(\lambda_1\) and \(\lambda_2\) be the eigenvalues of the associated characteristic equation \(\lambda^2-a\lambda-b=0\). 
                \2 If \(\lambda_1\neq \lambda_2\), then \(x_n=c_1\lambda_1^n+c_2\lambda_2^n\) for some scalars \(c_1\) and \(c_2\). 
                \2 If \(\lambda_1=\lambda_2=\lambda\), then \(x_n=c_1\lambda^n+c_2n\lambda^n\) for some scalars \(c_1\) and \(c_2\). 
            \1 In either case, \(c_1\) and \(c_2\) can be determined using the initial conditions. 
            \1 Let \(x_n=a_{m-1}x_{n-1}+a_{m-2}x_{n-2}+\cdots+a_0x_{n-m}\) be a recurrence relation of order \(m\) that is satisfied by a sequence \((x_n)\). Suppose the associated characteristic polynomial \[\lambda^m-a_{m-1}\lambda^{m-1}-a_{m-2}\lambda^{m-2}-\cdots-a_0\] factors as \((\lambda-\lambda_1)^{m_1}(\lambda-\lambda_2)^{m_2}\cdots(\lambda-\lambda_k)^{m_k}\), where \(m_1+m_2+\cdots+m_k=m\). Then \(x_n\) has the form \[x_n=(c_{11}\lambda_1^n+c_{12}n\lambda_1^n+c_{13}n^2\lambda_1^n+\cdots+c_{1m_1}n^{m_1-1}\lambda_1^n)+\cdots\]\[+(c_{k1}\lambda_k^n+c_{k2}n\lambda_k^n+c_{k3}n^2\lambda_k^n+\cdots+c_{km_k}n^{m_k-1}\lambda_k^n)\]
            \1 Let $A$ be an \(n\times n\) diagonalizable matrix and let \(P=[\vb v_1\quad\vb v_2\quad \cdots\quad \vb v_n]\) be such that \[P^{-1}AP=\begin{bmatrix}
                \lambda_1 & 0 & \cdots & 0\\ 0 & \lambda_2 & \cdots& 0\\\vdots &\vdots& \ddots &\vdots\\0&0&\cdots&\lambda_n
            \end{bmatrix}\] Then the general solution to the system \(\vb x'=A\vb x\) is \[\vb x=C_1e^{\lambda_1t}\vb v_1+C_2e^{\lambda_2t}\vb v_2+\cdots+C_ne^{\lambda_nt}\vb v_n\]
            \1 Let $A$ be an \(n\times n\) diagonalizable matrix with eigenvalues \(\lambda_1,\lambda_2,\cdots,\lambda_n\). Then the general solution to the system \(\vb x'=A\vb x\) is \(\vb x=e^{At}\vb c\), where \(\vb c\) is an arbitrary constant vector. If an initial condition \(\vb x(0)\) is specified, then \(\vb c=\vb x(0)\). 
            \1 Let \(A=\begin{bmatrix}
                a & -b \\ b & a
            \end{bmatrix}\). The eigenvalues of \(A\) are \(\lambda=a\pm bi\), and if \(a\) and \(b\) are not both zero, then $A$ can be factored as \[A=\begin{bmatrix}
                a & -b \\ b& a
            \end{bmatrix}=\begin{bmatrix}
                r & 0 \\ 0 & r
            \end{bmatrix}\begin{bmatrix}
                \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
            \end{bmatrix}\] where \(r=|\lambda|=\sqrt{a^2+b^2}\) and \(\theta\) is the principal argument of \(a+bi\)
            \1 Let $A$ be a real \(2\times 2\) matrix with a complex eigenvalue \(\lambda=a-bi\) (where \(b\neq 0\)) and corresponding eigenvector \(\vb x\). Then the matrix \(P=[\text{Re}\vb x\quad \text{Im}\vb x]\) is invertbible and \[A=P\begin{bmatrix}
                a & -b \\ b & a
            \end{bmatrix}P^{-1}\]
        \end{outline}
    \section{Orthogonality} % chapter title
        \subsection{Orthogonality in \(\mathbb R^n\)} % section topic
        \begin{outline}
            \1 A set of vectors \(\{\vb v_1,\vb v_2,\cdots,\vb v_k\}\) in \(\mathbb R^n\) is called an orthogonal set if all pairs of distinct vectors in the set are orthogonal; that is, if \[\vb v_i\cdot\vb v_j=0\quad\text{whenever}\quad i\neq j\quad\text{for }i,j=1,2,\cdots,k\]
            \1 The standard basis of \(\mathbb R^n\) is an orthogonal set. 
            \1 If \(\{\vb v_1,\vb v_2,\cdots,\vb v_k\}\) is an orthogonal set of nonzero vectors in \(\mathbb R^n\), then these vectors are linearly independent. 
            \1 An orthogonal basis for a subspace $W$ of \(\mathbb R^n\) is a basis of $W$ that is an orthogonal set. 
            \1 Let \(\{\vb v_1,\vb v_2,\cdots,\vb v_k\}\) be an orthogonal basis for a subspace $W$ of \(\mathbb R^n\) and let \(\vb w\) be any vector in $W$. Then the unique scalars \(c_1,\cdots,c_k\) such that \[\vb w=c_1\vb v_1+\cdots+c_k\vb v_k\] are given by \[c_i=\dfrac{\vb w\cdot\vb v_i}{\vb v_i\cdot \vb v_i}\qquad\text{for }i=1,\cdots,k\] 
            \1 A set of vectors \(\mathbb R^n\) is an orthonormal set if it is an orthogonal set of unit vectors. An orthonormal basis for a subspace $W$ of \(\mathbb R^n\) is a basis of $W$ that is an orthonormal set. 
            \1 Let \(\{\vb v_1,\vb v_2,\cdots,\vb v_k\}\) be an orthogonal basis for a subspace $W$ of \(\mathbb R^n\) and let \(\vb w\) be any vector in $W$. Then \[\vb w=(\vb w\cdot \vb q_1)\vb q_1+(\vb w\cdot\vb q_2)\vb q_2+\cdots+(\vb w\cdot\vb q_k)\vb q_k\] and this representation is unique. 
            \1 The columns of an \(m\times n\) matrix $Q$ form an orthonormal set IFF \(Q^TQ=I_n\)
            \1 An \(n\times n\) matrix $Q$ whose columns form an orthonormal set is called an orthogonal matrix. 
            \1 A square matrix \(Q\) is orthogonal IFF \(Q^{-1}=Q^T\)
            \1 Let $Q$ be an \(n\times n\) matrix. The following are equivalent:
                \2 $Q$ is orthogonal. 
                \2 \(\|Q\vb x\|=\|\vb x\|\) for every \(\vb x\) in \(\mathbb R^n\)
                \2 \(Q\vb x\cdot Q\vb y=\vb x\cdot\vb y\) for every \(\vb x\) and \(\vb y\) in \(\mathbb R^n\)
            \1 If $Q$ is an orthogonal matrix, then its rows form an orthonormal set. 
            \1 Let $Q$ be an orthogonal matrix. 
                \2 \(Q^{-1}\) is orthogonal. 
                \2 \(\det Q=\pm 1\)
                \2 If \(\lambda\) is an eigenvalue of $Q$, then \(|\lambda|=1\) 
                \2 If \(Q_1\) and \(Q_2\) are orthogonal \(n\times n\) matrices, then so is \(Q_1Q_2\). 

        \end{outline}
        \subsection{Orthogonal Complements and Orthogonal Projections}
        \begin{outline}
            \1 Let $W$ be a subspace of \(\mathbb R^n\). We say that a vector \(\vb v\) in \(\mathbb R^n\) is orthogonal to $W$ if \(\vb v\) is orthogonal to every vector in $W$. The set of all vectors that are orthogonal to $W$ is called the orthogonal complement of $W$, denoted \(W^{\perp}\). That is, \[W^\perp=\{\vb v\text{ in }\mathbb R^n:\vb v\cdot\vb w=0\quad \text{for all }\vb w\text{ in }W\}\]
            \1 Let $W$ be a subspace of \(\mathbb R^n\). 
                \2 \(W^\perp\) is a subspace of \(\mathbb R^n\)
                \2 \((W^\perp)^\perp=W\)
                \2 \(W\cap W^\perp=\{\vb 0\}\)
                \2 If \(W=\text{span}(\vb w_1,\ldots,\vb w_k)\), then $\vb v$ is in \(W^\perp\) IFF \(\vb v\cdot\vb w_i=0\) for all \(i=1,\ldots,k\). 
            \1 Let $A$ be an \(m\times n\) matrix. Then the orthogonal complement of the row space of $A$ is the null space of $A$, and the orthogonal complement of the column space of $A$ is the null space of \(A^T\): \[(\text{row}(A))^\perp=\text{null}(A)\quad\text{and}\quad(\text{col}(A))^\perp=\text{null}(A^T)\]
            \1 Let $W$ be a subspace of \(\mathbb R^n\) and let \(\{\vb u_1,\ldots,\vb u_k\}\) be an orthogonal basis for $W$. For any vector \(\vb v\) in \(\mathbb R^n\), the orthogonal projection of \(\vb v\) onto $W$ is defined as \[\text{proj}_w(\vb v)=\left(\dfrac{\vb u_1\cdot\vb v}{\vb u_1\cdot\vb u_1}\right)\vb u_1+\cdots+\left(\dfrac{\vb u_k\cdot \vb v}{\vb u_k\cdot\vb u_k}\right)\vb u_k\]The complement of \(\vb v\) orthogonal to $W$ is the vector \[\text{perp}_w(\vb v)=\vb v-\text{proj}_w(\vb v)\]
            \1 \(\text{proj}_w(\vb v)=\text{proj}_{\vb u_1}(\vb v)+\cdots+\text{proj}_{\vb u_k}(\vb v)\)
            \1 The orthogonal decomposition theorem: Let $W$ be a subspace of \(\mathbb R^n\) and let \(\vb v\) be a vector in \(\mathbb R^n\). Then there are unique vectors \(\vb w\) in $W$ and \(\vb w^\perp\) in \(W^\perp\) such that \[\vb v=\vb w+\vb w^\perp\]
            \1 If $W$ is a subspace of $\mathbb R^n$ then \[(W^\perp)^\perp=W\]
            \1 If $W$ is a subspace of $\mathbb R^n$ then \[\text{dim}W+\text{dim}W^\perp=n\]
            \1 The Rank Theorem: If $A$ is an \(m\times n\) matrix, then \[\text{rank}(A)+\text{nullity}(A)=n\]


        \end{outline}
        \subsection{The Gram-Schmidt Process and the \(QR\) Factorization}
        \begin{outline}
            \1 The Gram-Schmidt Process: Let \(\{\vb x_1,\ldots,\vb x_k\}\) be a basis for a subspace $W$ of \(\mathbb R^n\) and define the following: \[\vb v_1=\vb x_1;\qquad\qquad W_1=\text{span}(\vb x_1)\]\[\vb v_2=\vb x_2-\left(\dfrac{\vb v_1\cdot\vb x_2}{\vb v_1\cdot\vb v_1}\right)\vb v_1,\qquad\qquad W_2=\text{span}(\vb x_1,\vb x_2)\]\[\vb v_3=\vb x_3-\left(\dfrac{\vb v_1\cdot\vb x_3}{\vb v_1\cdot\vb v_1}\right)\vb v_1-\left(\dfrac{\vb v_2\cdot\vb x_3}{\vb v_2\cdot\vb v_2}\right)\vb v_2,\qquad\qquad W_3=\text{span}(\vb x_1,\vb x_2,\vb x_3)\]\[\vdots\]\[\vb v_k=\vb x_k-\left(\dfrac{\vb v_1\cdot\vb x_k}{\vb v_1\cdot=v_1}\right)\vb v_1-\left(\dfrac{\vb v_2\cdot\vb x_k}{\vb v_2\cdot\vb v_2}\right)\vb v_2-\cdots-\left(\dfrac{\vb v_{k-1}\cdot\vb x_k}{\vb v_{k-1}\cdot\vb v_{k-1}}\right)\vb v_{k-1},\qquad\qquad W_k=\text{span}(\vb x_1,\ldots,\vb x_k)\] Then for each \(i=1,\ldots,k,\{\vb v_1,\ldots,\vb v_i\}\) is an orthogonal basis for \(W_i\). In particular, \(\{\vb v_1,\ldots,\vb v_i\}\) is an orthogonal basis for $W$. 
            \1 QR Factorization: Let $A$ be an \(m\times n\) matrix with linearly independent columns. Then $A$ can be factored as $A=QR$, where $Q$ is an \(m\times n\) matrix with orthonormal columns and $R$ is an invertible upper triangular matrix. 
            \1 Finding the QR factorization: find an orthonormal basis for \(\text{col}(A)\) using the Gram-Schmidt Process. Then, \(Q=[\vb q_1,\vb q_2,\ldots,\vb q_k]\). Then, use the fact that \(A=QR\) and \(Q^TQ=I\) since $Q$ has orthonormal columns. Therefore \(Q^TA=Q^TQR=IR=R\)
        \end{outline}
        \subsection{Orthogonal Diagonalization of Symmetric Matrices}
        \begin{outline}
            \1 A square matrix $A$ is orthogonally diagonalizable if there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that \(Q^TAQ=D\)
            \1 If $A$ is orthogonally diagonalizable, then $A$ is symmetric. 
            \1 If $A$ is a real symmetric matrix, then the eigenvalues of $A$ are real. 
            \1 If $A$ is a symmetric matrix, then any two eigenvectors corresponding to distinct eigenvalues of $A$ are orthonormal. 
            \1 The spectral theorem: Let $A$ be an \(n\times n\) real matrix. Then $A$ is symmetric IFF it is orthogonally diagonalizable. 
            \1 Spectral decomposition: \[A=\lambda_1\vb q_1\vb q_1^T+\lambda_2\vb q_2\vb q_2^T+\cdots+\lambda_n\vb q_n\vb q_n^T\]

        \end{outline}
        \subsection{Applications}
        \begin{outline}
            \1 A quadratic form in $n$ variables is a function \(f:\mathbb R^n\to\mathbb R\) of the form \[f(\vb x)=\vb x^TA\vb x\] where $A$ is a symmetric \(n\times n\) matrix and \(\vb x\) is in \(\mathbb R^n\). We refer to $A$ as the matrix associated with $f$. 
            \1 The principal axes theroem: Every quadratic form can be diagonalized. Specifically, if $A$ is the \(n\times n\) symmetric matrix associated with the quadratic form \(\vb x^TA\vb x\) and if $Q$ is an orthogonal matrix such that \(Q^TAQ=D\) is a diagonal matrix, then the change of variable \(\vb x=Q\vb y\) transforms the quadratic form \(\vb x^TA\vb x\) into the quadratic form \(\vb y^TD\vb y\), which has no cross-product terms. If the eigenvalues of $A$ are \(\lambda_1,\ldots,\lambda_n\) and \(\vb y=[y_1\cdots y_n]^T\), then \[\vb x^TA\vb x=\vb y^TD\vb y=\lambda_1y_a^2+\cdots+\lambda_ny_n^2\]
            \1 A quadratic form \(f(\vb x)=\vb x^TA\vb x\) is classified as one of the following: 
                \2 positive definite if \(f(\vb x)>0\) for all \(\vb x\neq\vb 0\)
                \2 positive semidefinite if \(f(\vb x)\geq0\) for all \(\vb x\)
                \2 negative definite if \(f(\vb x)<0\) for all \(\vb x\neq\vb 0\)
                \2 negative semidefinite if \(f(\vb x)\leq0\) for all \(\vb x\)
                \2 indefinite if \(f(\vb x)\) takes on both positive and negative values
            \1 A symmetric matrix $A$ is called positive definite, positive semidefinite, negative definite, negative semidefinite, or indefinite if the associated quadratic form \(f(\vb x)=\vb x^TA\vb x\) has the corresponding property. 
            \1 Let $A$ be an \(n\times n\) symmetric matrix. The quadratic form \(f(\vb x)=\vb x^TA\vb x\) is 
                \2 Positive definite IFF all eigenvalues of $A$ are positive. 
                \2 positive semidefinite IFF all eigenvalues are nonnegative. 
                \2 negative definite IFF all eigenvalues are negative
                \2 negative semidefinite IFF all eigenvalues are nonpositive. 
                \2 indefinite IFF $A$ has both positive and negative eigenvalues. 
            \1 Let \(f(\vb x)=\vb x^TA\vb x\) be a quadratic form with associated \(n\times n\) symmetric matrix $A$. Let the eigenvalues of $A$ be \(\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n\). Then the following are true, with the constraint of \(\|\vb x\|=1\): 
                \2 \(\lambda_1\geq f(\vb x)\geq\lambda_n\)
                \2 The max value of \(f(\vb x)\) is \(\lambda_1\) and occurs when \(\vb x\) is a unit eigenvector corresponding to \(\lambda_1\)
                \2 The min value of \(f(\vb x)\) is \(\lambda_n\) and occurs when \(\vb x\) is a unit eigenvector corresponding to \(\lambda_n\)
            \1 The general form of a quadratic equation in two variables $x$ and $y$ is \[ax^2+by^2+cxy+dx+ey+f=0\]
            \1 The general form of a quadratic equation in three variables $x$, $y$, and $z$ is \[ax^2+by^2+cz^2+dxy+exz+fyz+gx+hy+iz+j=0\]

        \end{outline}
    \section{Vector Spaces}
        \subsection{Vector Spaces and Subspaces}
        \begin{outline}
            \1 In the past, we studied vectors in a concrete situation, \(\mathbb R^n\). Now, we generalize ``vectors'' by abstracting them into a general setting. 
            \1 Let $V$ be a set on which two operations, called addition and scalar multiplication, have been defined. If \(\vb u\) and \(\vb v\) are in $V$, the sum of \(\vb u\) and \(\vb v\) is denoted by \(\vb u+\vb v\), and if $c$ is a scalar, the scalar multiple of $\vb u$ by $c$ is denoted by \(c\vb u\). If the following axioms hold for all \(\vb u\), \(\vb v\), and \(\vb w\) in $V$ and for all scalars \(c\) and \(d\), then $V$ is called a vector space and its elements are vectors. 
        \end{outline}
        \begin{enumerate}
            \item \(\vb u+\vb v\) is in $V$. 
            \item \(\vb u+\vb v=\vb v+\vb u\)
            \item \((\vb u+\vb v)+\vb w=\vb u+(\vb v+\vb w)\)
            \item There exists an element $\vb 0$ in $V$, called a zero vector, such that \(\vb u+\vb 0=\vb u\)
            \item For each \(\vb u\) in $V$, there is an element \(-\vb u\) in $V$ such that \(\vb u+(-\vb u)=\vb 0\)
            \item \(c\vb u\) is in $V$. 
            \item \(c(\vb u+\vb v)=c\vb u+c\vb v\)
            \item \((c+d)\vb u=c\vb u+d\vb u\)
            \item \(c(d\vb u)=(cd)\vb u\)
            \item \(1\vb u=\vb u\)
        \end{enumerate}
        \begin{outline}
            \1 Let $V$ be a vector space \(\vb u\) a vector in $V$, and $c$ a scalar. 
                \2 \(0\vb u=\vb 0\)
                \2 \(c\vb 0=\vb 0\)
                \2 \((-1)\vb u=-\vb u\)
                \2 If \(c\vb u=\vb 0\), then \(c=0\) or \(\vb u=\vb 0\)
            \1 A subset $W$ of a vector space $V$ is called a subspace of $V$ if $W$ is itself a vector space with the same scalars, addition, and scalar multiplication as $V$. 
            \1 Let $V$ be a vector space and let $W$ be a nonempty subset of $V$. Then $W$ is a subspace of $V$ IFF the following conditions hold: 
                \2 If \(\vb u\) and \(\vb v\) are in $W$, then \(\vb u+\vb v\) is in \(W\)
                \2 If \(\vb u\) is in $W$ and $c$ is a scalar, then \(c\vb u\) is in $W$. 
            \1 If $W$ is a subspace of a vector space $V$, then $W$ contains the zero vector \(\vb 0\) of $V$. 
            \1 If \(S=\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) is a set of vectors in a vector space $V$, then the set of all linear combinations of \(\vb v_1,\vb v_2,\ldots,\vb v_k\) is called the span of \(\vb v_1,\vb v_2,\ldots,\vb v_k\) and is denoted by \(\text{span}(\vb v_1,\vb v_2,\ldots,\vb v_k)\) or \(\text{span}(S)\). If \(V=\text{span}(S)\), then $S$ is called a spanning set of $V$ and $V$ is said to be spanned by $S$. 
            \1 Let \(\vb v_1,\vb v_2,\ldots,\vb v_k\) be vectors in a vector space $V$. 
                \2 \(\text{span}(\vb v_1,\vb v_2,\ldots,\vb v_k)\) is a subspace of $V$. 
                \2 \(\text{span}(\vb v_1,\vb v_2,\ldots,\vb v_k)\) is the smallest subspace of $V$ that contains \(\vb v_1,\vb v_2,\ldots,\vb v_k\)

        \end{outline}
        \subsection{Linear Independence, Basis, and Dimension}
        \begin{outline}
            \1 A set of vectors \(\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) in a vector space $V$ is linearly dependent if there are scalars \(c_1,c_2,\ldots,c_k\), at least one of which is not zero, such that \[c_1\vb v_1+c_2\vb v_2+\cdots+c_k\vb v_k=\vb 0\] A set of vectors that is not linearly dependent is said to be linearly independent. 
            \1 A set of vectors \(\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) in a vector space $V$ is linearly dependent IFF at least one of the vectors can be expressed as a linear combination of the others. 
            \1 A set $S$ of vectors in a vector space $V$ is linearly dependent if it contains finitely many linearly dependent vectors. A set of vectors that is not linearly dependent is said to be linearly independent. 
            \1 A subset \(\mathcal B\) of a vector space $V$ is a basis for $V$ if 
                \2 \(\mathcal B\) spans $V$ and 
                \2 \(\mathcal B\) is linearly indepnedent. 
            \1 Let $V$ be a vector space and let \(\mathcal B\) be a basis for $V$. For every vector \(\vb v\) in $V$, there is exactly one way to write $\vb v$ as a linear combination of the basis vectors in \(\mathcal B\)
            \1 Let \(\mathcal B=\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) be a basis for the vector space $V$. Let \(\vb v\) be a vector in $V$, and write \(\vb v=c_1\vb v_1+c_2\vb v_2+\cdots+c_n\vb v_n\). Then \(c_1,c_2,\ldots,c_n\) are called the coordinates of \(\vb v\) with respect to \(\mathcal B\), and the column vector \[[\vb v]_{\mathcal B}=\begin{bmatrix}
                c_1\\c_2\\\vdots\\c_n
            \end{bmatrix}\] is called the coordinate vector of \(\vb v\) with respect to \(\mathcal B\). 
            \1 Let \(\mathcal B=\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) be a basis for a vector space $V$. Let \(\vb u\) and \(\vb v\) be vectors in $V$ and let \(c\) be a scalar. Then 
                \2 \([\vb u+\vb v]_\mathcal B=[\vb u]_\mathcal B+[\vb v]_\mathcal B\)
                \2 \([c\vb u]_\mathcal B=c[\vb u]_\mathcal B\)
            \1 \[[c_1\vb u_1+\cdots+c_k\vb u_k]_\mathcal B=c_1[\vb u_1]_\mathcal B+\cdots+c_k[\vb u_k]_\mathcal B\]
            \1 Let \(\mathcal B=\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) be a basis for a vector space $V$ and let \(\vb u_1,\ldots,\vb u_k\) be vectors in $V$. Then \(\{\vb u_1,\ldots,\vb u_k\}\) is linearly independent in $V$ IFF \(\{[\vb u_1]_\mathcal B,\ldots,[\vb u_k]_\mathcal B\}\) is linearly independent in \(\mathbb R^n\). 
            \1 Let \(\mathcal B=\{\vb v_1,\vb v_2,\ldots,\vb v_k\}\) be a basis for a vector space $V$. 
                \2 Any set of more than $n$ vectors in $V$ must be linearly dependent. 
                \2 Any set of fewer than $n$ vectors in $V$ cannot span $V$. 
            \1 The Basis Theorem: If a vector space $V$ has a basis with $n$ vectors, then every basis for $V$ has exactly $n$ vectors. 
            \1 A vector space $V$ is called finite-dimensional if it has a basis consisting of finitely many vectors. The dimension of $V$, denoted by \(\text{dim }V\), is the number of vectors in a basis for $V$. The dimension of the zero vector space \(\{\vb 0\}\) is defined to be zero. A vector space that has no finite basis is called infinite-dimensional. 
            \1 Let $V$ be a vector space with \(\text{dim }V=n\). Then: 
                \2 Any linearly independent set in $V$ contains at most $n$ vectors. 
                \2 Any spanning set for $V$ contains at least $n$ vectors. 
                \2 Any linearly indpendent set of exactly $n$ vectors in $V$ is a basis for $V$. 
                \2 Any spanning set for $V$ consisting of exactly $n$ vectors is a basis for $V$. 
                \2 Any linearly independent set in $V$ can be extended to a basis for $V$. 
                \2 Any spanning set for $V$ can be reduced to a basis for $V$. 
            \1 Let $W$ be a subspace of a finite-dimensional vector space $V$. Then: 
                \2 $W$ is finite-dimensional and \(\text{dim }W\leq\text{dim }V\). 
                \2 \(\text{dim }W=\text{dim }V\) IFF \(W=V\)
        \end{outline}
        \subsection{Change of Basis}
        \begin{outline}
            \1 Let \(\mathcal B=\{\vb u_1,\ldots,\vb u_n\}\) and \(\mathcal C=\{\vb v_1,\ldots,\vb v_n\}\) be bases for a vector space $V$. The \(n\times n\) matrix whose columns are the coordinate vectors \([\vb u_1]_\mathcal C,\ldots,[\vb u_n]_\mathcal C\) of the vectors in \(\mathcal B\) with respect to \(\mathcal C\) is denoted by \(P_{\mathcal C\leftarrow\mathcal B}\) and is called the change-of-basis matrix from \(\mathcal B\) to \(\mathcal C\). That is, \[P_{\mathcal C\leftarrow\mathcal B}=[[\vb u_1]_\mathcal C[\vb u_2]_\mathcal C\cdots[\vb u_n]_\mathcal C]\]
            \1 Let \(\mathcal B=\{\vb u_1,\ldots,\vb u_n\}\) and \(\mathcal C=\{\vb v_1,\ldots,\vb v_n\}\) be bases for a vector space $V$ and let \(P_{\mathcal C\leftarrow\mathcal B}\) be the change-of-basis matrix from \(\mathcal B\) to \(\mathcal C\). Then 
                \2 \(P_{\mathcal C\leftarrow\mathcal B[\vb x]_\mathcal B=[\vb x]_\mathcal{C}}\) for all \(\vb x\) in $V$. 
                \2 \(P_{\mathcal C\leftarrow\mathcal B}\) is the unique matrix $P$ with the property that \(P[\vb x]_\mathcal B=[\vb x]_\mathcal C\) for all \(\vb x\) in $V$. 
                \2 \(P_{\mathcal C\leftarrow\mathcal B}\) is invertible and \((P_{\mathcal C\leftarrow\mathcal B})^{-1}=P_{\mathcal B\leftarrow\mathcal C}\). 
            \1 Let \(\mathcal B=\{\vb u_1,\ldots,\vb u_n\}\) and \(\mathcal C=\{\vb v_1,\ldots,\vb v_n\}\) be bases for a vector space $V$. Let \(B=[[\vb u_1]_\mathcal E\cdots[\vb u_n]_\mathcal E]\) and \(C=[[\vb v_1]_\mathcal E\cdots[\vb v_n]_\mathcal E]\), where \(\mathcal E\) is any basis for $V$. Then the row reduction applied to the \(n\times 2n\) augmented matrix \([C|B]\) produces \[[C|B]\to[I|P_{\mathcal C\leftarrow\mathcal B}]\]

        \end{outline}
        \subsection{Linear Transformations}
        \begin{outline}
            \1 A linear transformation from a vector space $V$ to a vector space $W$ is a mapping \(T:V\to W\) such that, for all \(\vb u\) and \(\vb v\) in $V$ and for all scalars $c$, 
                \2 \(T(\vb u+\vb v)=T(\vb u)+T(\vb v)\)
                \2 \(T(c\vb u)=cT(\vb u)\)
            \1 \(T:V\to W\) is a linear transformation IFF \[T(c_1\vb v_1+c_2\vb v_2+\cdots+c_k\vb v_k)=c_1T(\vb v_1)+c_2T(\vb v_2)+\cdots+c_kT(\vb v_k)\] for all \(\vb v_1,\ldots,\vb v_k\) in $V$ and scalars \(c_1,\ldots,c_k\). 
            \1 Let \(T:V\to W\) be a linear transformation. Then: 
                \2 \(T(\vb 0)=\vb 0\)
                \2 \(T(-\vb v)=-T(\vb v)\) for all \(\vb v\) in $V$. 
                \2 \(T(\vb u-\vb v)=T(\vb u)-T(\vb v)\) for all \(\vb u\) and \(\vb v\) in $V$. 
            \1 Let \(T:V\to W\) be a linear transformation and let \(\mathcal B=\{\vb v_1,\ldots,\vb v_n\}\) be a spanning set for $V$. Then \(T(\mathcal B)=\{T(\vb v_1),\ldots,T(\vb v_n)\}\) spans the range of $T$. 
            \1 If \(T:U\to V\) and \(S:V\to W\) are linear transformations, then the composition of $S$ with $T$ is the mapping \(S\circ T\), defined by \[(S\circ T)(\vb u)=S(T(\vb u))\] where \(\vb u\) is in $U$. 
            \1 If \(T:U\to V\) and \(S:V\to W\) are linear transformations, then \(S\circ T:U\to W\) is a linear transformation. 
            \1 \(R\circ(S\circ T)=(R\circ S)\circ T\)
            \1 A linear transformation \(T:V\to W\) is invertible if there is a linear transformation \(T':W\to V\) such that \[T'\circ T=I_V\quad\text{and}\quad T\circ T'=I_W\] In this case, \(T'\) is called an inverse for $T$. 
            \1 If $T$ is an invertible linear transformation, then its inverse is unique. 
        \end{outline}
        \subsection{The Kernel and Range of a Linear Transformation}
        \begin{outline}
            \1 Let \(T:V\to W\) be a linear transformation. The kernel of $T$, denoted \(\text{ker}(T)\), is the set of all vectors in $V$ that are mapped by $T$ to \(\vb 0\) in $W$. That is, \[\text{ker}(T)=\{\vb v\text{ in }V:T(\vb v)=\vb 0\}\] The range of $T$, denoted \(\text{range}(T)\), is the set of all vectors in $W$ that are images of vectors in $V$ under $T$. That is, \[\text{range}(T)=\{T(\vb v):\vb v\text{ in }V\}=\{\vb w\text{ in }W:\vb w=T(\vb v)\text{ for some }\vb v\text{ in }V\}\]
            \1 Let \(T:V\to W\) be a linear transformation. Then: 
                \2 The kernel of $T$ is a subspace of $V$. 
                \2 The range of $T$ is a subspace of $W$. 
            \1 Let \(T:V\to W\) be a linear transformation. The rank of $T$ is the dimension of the range of $T$ and is denoted by \(\text{rank}(T)\). The nullity of $T$ is the dimension of the kernel of $T$ and is denoted by \(\text{nullity}(T)\). 
            \1 The rank theorem: Let \(T:V\to W\) be a linear transformation from a finite-dimensional vector space $V$ into a vector space $W$. Then \[\text{rank}(T)+\text{nullity}(T)=\text{dim }V\]
            \1 A linear transformation \(T:V\to W\) is called one-to-one if $T$ maps distinct vectors in $V$ to distinct vectors in W. If \(\text{range}(T)=W\), then $T$ is called onto. 
            \1 \(T:V\to W\) is one-tocloftone if, for all \(\vb u\) and \(\vb v\) in $V$, \[\vb u\neq\vb v\text{ implies that }T(\vb u)\neq T(\vb v)\]
            \1 Which is to say, if \(T:V\to W\) is one-to-one if, for all \(\vb u\) and \(\vb v\) in $V$, \[T(\vb u)=T(\vb v)\text{ implies that }\vb u=\vb v\]
            \1 \(T:V\to W\) is onto if, for all \(\vb w\) in $W$, there is at leeast one \(\vb v\) in $V$ such that \[\vb w=T(\vb v)\]
            \1 A linear transformation \(T:V\to W\) is one-to-one IFF \(\text{ker}(T)=\{\vb 0\}\). 
            \1 Let \(\text{dim }V=\text{dim }W=n\). Then a linear transformation \(T:V\to W\) is one-to-one IFF it is onto
            \1 Let \(T:V\to W\) be a one-to-one linear transformation. If \(S=\{\vb v_1,\ldots,\vb v_k\}\) is a linearly indepnedent set in $V$, then \(T(S)=\{T(\vb v_1),\ldots,T(\vb v_k)\}\) is a linearly independent set in $W$. 
            \1 Let \(\text{dim }V=\text{dim }W=n\). Then a one-to-one linear transformation \(T:V\to W\) maps a basis for $V$ to a basis for $W$. 
            \1 A linear transformation \(T:V\to W\) is invertible IFF it is one-to-one and onto. 
            \1 A lineart transformation \(T:V\to W\) is called an isomorphism if it is one-to-one and onto. If $V$ and $W$ are two vector spaces such that there is an isomorphism from $V$ to $W$, then we say that $V$ is isomorphic to $W$ and write \(V\cong W\). 
            \1 Let $V$ and $W$ be two finite-dimensional vector spaces (over the same field of scalars). Then $V$ is isomorphic to $W$ IFF \(\text{dim }V=\text{dim }W\). 

        \end{outline}
        \subsection{The Matrix of a Linear Transformation}
        \begin{outline}
            \1 Let $V$ and $W$ be two finite-dimensional vector spaces with bases \(\mathcal B\) and \(\mathcal C\), respectively, where \(\mathcal B=\{\vb v_1,\ldots,\vb v_n\}\). If \(T:V\to W\) is a linear transformation, then the \(m\times n\) matrix $A$ defined by \[A=[[T(\vb v_1)]_\mathcal C|[T(\vb v_2)]_\mathcal C|\cdots|[T(\vb v_n)]_\mathcal C]\] satisfies \[A[\vb v]_\mathcal B=[T(\vb v)]_\mathcal C\] for every vector \(\vb v\) in $V$. 
            \1 \([T]_{\mathcal C\leftarrow\mathcal B}[\vb v]_\mathcal B=[T(\vb v)]_\mathcal C\)
            \1 \([T]_\mathcal B[\vb v]_\mathcal B=[T(\vb v)]_\mathcal B\)
            \1 Let $U$, $V$, and $W$ be finite-dimensional vector spaces with bases \(\mathcal B\), \(\mathcal C\), and \(\mathcal D\), respectively. Let \(T:U\to V\) and \(S:V\to W\) be linear transformations. Then \[[S\circ T]_{\mathcal D\leftarrow\mathcal B}=[S]_{\mathcal D\leftarrow\mathcal C}[T]_{\mathcal C\leftarrow\mathcal B}\]
            \1 Let \(T:V\to W\) be a linear transformation between $n$-dimensional vector spaces $V$ and $W$ and let \(\mathcal B\) and \(\mathcal C\) be bases for $V$ and $W$, respectively. Then $T$ is invertible IFF the matrix \([T]_{\mathcal C\leftarrow\mathcal B}\) is invertible. In this case, \[([T]_{\mathcal C\leftarrow\mathcal B})^{-1}=[T^{-1}]_{\mathcal B\leftarrow\mathcal C}\]
            \1 Let $V$ be a finite dimensional vector space with bases \(\mathcal B\) and \(\mathcal C\) and let \(T:V\to V\) be a linear transformation. Then \[[T]_\mathcal C=P^{-1}[T]_\mathcal BP\] where $P$ is the change-of-basis matrix from \(\mathcal C\) to \(\mathcal B\). 
            \1 Let $V$ be a finite-dimensional vector space and let \(T:V\to V\) be a linear transformation. Then $T$ is called diagonalizable if there is a basis \(\mathcal C\) for $V$ such that the matrix \([T]_\mathcal C\) is a diagonal matrix. 
            \1 The Fundamental Theorem of invertible matrices: version 4. 
                \2 $A$ is invertible
                \2 $A\vb x=\vb b$ has a unique solution for every \(\vb b\) in \(\mathbb R^n\)
                \2 \(A\vb x=\vb 0\) has only the trivial solution. 
                \2 The reduced row echelon form of $A$ is \(I_n\). 
                \2 $A$ is the product of elementary matrices. 
                \2 \(\rank (A)=n\)
                \2 nullity\((A)=0\)
                \2 The column vectors of $A$ are linearly independent
                \2 The column vectors of $A$ span \(\mathbb R^n\)
                \2 The column vectors of $A$ form a basis for \(\mathbb R^n\)
                \2 The row vectors of $A$ are linearly independent
                \2 The row vectors of $A$ span \(\mathbb R^n\)
                \2 The row vectors of $A$ form a basis for \(\mathbb R^n\)
                \2 \(\det A\neq 0\)
                \2 $0$ is not an eigenvalue of $A$
                \2 $T$ is invertible. 
                \2 $T$ is one-to-one. 
                \2 $T$ is onto. 
                \2 \(\text{ker}(T)=\{\vb 0\}\)
                \2 \(\text{range}(T)=W\)
        \end{outline}
        \subsection{Applications}
        \begin{outline}
            \1 The set $S$ of all solutions to \(y'+ay=0\) is a subspace of \(\mathscr F\)
            \1 If $S$ is the solution space of \(y'+ay=0\), then \(\text{dim }S=1\) and \(\{e^{-at}\}\) is a basis for $S$. 
            \1 Let $S$ be the solution space of \[y''+ay'+by=0\] and let \(\lambda_1\) and \(\lambda_2\) be the roots of the characteristic equation \(\lambda^2+a\lambda+b=0\). 
                \2 If \(\lambda_1\neq\lambda_2\), then \(\{e^{\lambda_1t},e^{\lambda_2t}\}\) is a basis for $S$. 
                \2 If \(\lambda_1=\lambda_2\), then \(\{e^{\lambda_1t},te^{\lambda_1t}\}\) is a basis for $S$. 

        \end{outline}
    \section{Distance and Approximation} % chapter title
        \subsection{Inner Product Spaces} 
        \begin{outline}
            \1 An inner product on a vector space $V$ is an operation that assigns to every pair of vectors \(\vb u\) and \(\vb v\) in $V$ a real number \(\langle\vb u,\vb v\rangle\) such that the following properties hold for all vectors \(\vb u\), \(\vb v\), and \(\vb w\) in $V$ and all scalars $c$:
                \2 \(\langle\vb u,\vb v\rangle=\langle\vb v,\vb u\rangle\)
                \2 \(\langle\vb u,\vb v+\vb w\rangle=\langle\vb u,\vb v\rangle+\langle\vb u,\vb w\rangle\)
                \2 \(\langle c\vb u,\vb v\rangle=c\langle\vb u,\vb v\rangle\)
                \2 \(\langle\vb u,\vb u\rangle\geq 0\) and \(\langle\vb u,\vb u\rangle=0\) IFF \(\vb u=\vb 0\)
            \1 A vector space with an inner product is called an inner product space. 
            \1 Let \(\vb u\), \(\vb v\), and \(\vb w\) be vectors in an inner product space $V$ and let \(c\) be a scalar. 
                \2 \(\langle\vb u+\vb v,\vb w\rangle=\langle\vb u,\vb w\rangle+\langle\vb v+\vb w\rangle\)
                \2 \(\langle\vb u,c\vb v\rangle=c\langle\vb u,\vb v\rangle\)
                \2 \(\langle\vb u,\vb 0\rangle=\langle\vb 0,\vb v\rangle=0\)
            \1 Let \(\vb u\) and \(\vb v\) be vectors in an inner product space $V$. 
                \2 The length (or norm) of \(\vb v\) is \(\|\vb v\|=\sqrt{\langle\vb v,\vb v\rangle}\). 
                \2 The distance between \(\vb u\) and \(\vb v\) is \(d(\vb u,\vb v)=\|\vb u-\vb v\|\)
                \2 \(\vb u\) and \(\vb v\) are orthogonal if \(\langle\vb u,\vb v\rangle=0\). 
            \1 Pythagoras' Theorem: Let \(\vb u\) and \(\vb v\) be vectors in an inner product space $V$. Then \(\vb u\) and \(\vb v\) are orthogonal IFF \[\|\vb u+\vb v\|^2=\|\vb u\|^2+\|\vb v\|^2\]
            \1 The Cauchy-Schwarz Inequality: Let \(\vb u\) and \(\vb v\) be vectors in an inner product space $V$. Then \[|\langle\vb u,\vb v\rangle|\leq\|\vb u\\\|\vb v\|\] with equality holding IFF $\vb u$ and $\vb v$ are scalar multiples of each other. 
        \1 The triangle inequality: Let $\vb u$ and $\vb v$ be vectors in an inner product space $V$. Then \[\|\vb u+\vb v\|\leq\|\vb u\|+\|\vb v\|\]
        \end{outline}
        \subsection{Norms and Distance Functions} 
        \begin{outline}
            \1 A norm on a vector space $V$ is a mapping that associates with each vector $\vb v$ a real number $\|\vb v\|$, called the norm of $\vb v$, such that the following properties are satisfied for all vectors $\vb u$ and $\vb v$ and all scalars $c$:
            \2 \(\|\vb v\|\geq 0\), and \(\|\vb v\|=0\) IFF \(\vb v=\vb 0\)
            \2 \(\|c\vb v\|=|c\\|\vb v\|\)
            \2 \(\|\vb u+\vb v\|\leq\|\vb u\|+\|\vb v\|\)
        \1 A vector space with a norm is called a normed vector space. 
        \1 We define a distance function for any norm as: \[d(\vb u,\vb v)=\|\vb u-\vb v\|\]
        \1 Let $d$ be a distance function defined on a normed linear space $V$. The following properties hold for all vectors $\vb u$, $\vb v$, and $\vb w$ in $V$:
            \2 \(d(\vb u,\vb v)\geq 0\), and \(d(\vb u,\vb v)=0\) IFF \(\vb u=\vb v\)
            \2 \(d(\vb u,\vb v)=d(\vb v,\vb u)\) 
            \2 \(d(\vb u,\vb w)\leq d(\vb u,\vb v)+d(\vb v,\vb w)\)
        \1 A matrix norm on \(M_{nn}\) is a mapping that associates with each \(n\times n\) matrix $A$ a real number $\|A\|$, called the norm of $A$, such that the following properties are satisfied for all \(n\times n\) matrices $A$ and $B$ and all scalars $c$. 
            \2 \(\|A\|\geq 0\) and \(\|A\|=0\) IFF \(A=O\). 
            \2 \(\|cA\|=|c\\|A\|\)
            \2 \(\|A+B\|\leq\|A\|+\|B\|\)
            \2 \(\|AB\|\leq\|A\|\|B\|\)
        \1 A matrix norm on $M_{nn}$ is said to be compatible with a vector norm on \(\|\vb x\|\) on \(\mathbb R^n\) if, for all \(n\times n\) matrices $A$ and all vectors $\vb x$ in \(\mathbb R^n\), we have \[\|A\vb x\|\leq\|A\|\|\vb x\|\]
        \1 The Frobenius norm is given by \[\|A\|_F=\sqrt{\sum^n_{i,j=1}a^2_{ij}}\]
        \1 If \(\|\vb x\|\) is a vector norm on \(\mathbb R^n\), then \(\|A\|=\max_{\|\vb x\|=1}\|A\vb x\|\) defines a matrix norm on $M_{nn}$ that is compatible with the vector norm that induces it. 
        \1 The matrix norm $\|A\|$ in the previous is called the operator norm induced by the vector norm \(\|\vb x\|\)
        \1 Let $A$ be an \(n\times n\) matrix with column vectors \(\vb a_i\) and row vectors $\vb A_i$ for \(i=1,\ldots,n\). \[\text{a. }\|A\|_{1}=\max _{j=1, \ldots, n}\left\{\left\|\mathbf{a}_{j}\right\|_{s}\right\}=\max _{j=1, \ldots, n}\left\{\sum_{i=1}^{n}\left|a_{i j}\right|\right\}\]\[\text{b. }\|A\|_{\infty}=\max _{i=1, \ldots, n}\left\{\left\|\mathbf{A}_{i}\right\|_{s}\right\}=\max _{i=1, \ldots, n}\left\{\sum_{j=1}^{n}\left|a_{i j}\right|\right\}\]
        \1 A matrix $A$ is ill-conditioned if small changes in its entries can produce large changes in the solutions to $A \mathbf{x}=\mathbf{b}$. If small changes in the entries of $A$ produce only small changes in the solutions to $A \mathbf{x}=\mathbf{b}$, then $A$ is called well-conditioned.
    
        \end{outline}
        \subsection{Least Squares Approximation} 
        \begin{outline}
            \1 If $A$ is an $m \times n$ matrix and $\mathbf{b}$ is in $\mathbb{R}^{m}$, a least squares solution of $\overline{A \mathbf{x}}=\mathbf{b}$ is a vector $\overline{\mathbf{x}}$ in $\mathbb{R}^{n}$ such that \[\|\mathbf{b}-A \overline{\mathbf{x}}\| \leq\|\mathbf{b}-A \mathbf{x}\|\] for all $\mathbf{x}$ in $\mathbb{R}^{n}$.
            \1 The Least Squares Theorem: Let $A$ be an $m \times n$ matrix and let $\mathbf{b}$ be in $\mathbb{R}^{m}$. Then $A \mathbf{x}=\mathbf{b}$ always has at least one least squares solution $\overline{\mathbf{x}}$. Moreover:
                \2 $\overline{\mathbf{x}}$ is a least squares solution of $A \mathbf{x}=\mathbf{b}$ if and only if $\overline{\mathbf{x}}$ is a solution of the normal equations $A^{T} A \overline{\mathbf{x}}=A^{T} \mathbf{b}$.
                \2 A has linearly independent columns if and only if $A^{T} A$ is invertible. In this case, the least squares solution of $A \mathbf{x}=\mathbf{b}$ is unique and is given by
                \[\overline{\mathbf{x}}=\left(A^{T} A\right)^{-1} A^{T} \mathbf{b}\]
            \1 Let $A$ be an $m \times n$ matrix with linearly independent columns and let $\mathbf{b}$ be in $\mathbb{R}^{m}$. If $A=Q R$ is a $Q R$ factorization of $A$, then the unique least squares solution $\overline{\mathbf{x}}$ of $A \mathbf{x}=\mathbf{b}$ is
            \[\overline{\mathbf{x}}=R^{-1} Q^{T} \mathbf{b}\]
            \1 Let $W$ be a subspace of $\mathbb{R}^{m}$ and let $A$ be an $m \times n$ matrix whose columns form a basis for $W$. If $\mathbf{v}$ is any vector in $\mathbb{R}^{m}$, then the orthogonal projection of $\mathbf{v}$ onto $W$ is the vector
            \[\operatorname{proj}_{W}(\mathbf{v})=A\left(A^{T} A\right)^{-1} A^{T} \mathbf{v}\]
            The linear transformation $P: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ that projects $\mathbb{R}^{m}$ onto $W$ has $A\left(A^{T} A\right)^{-1} A^{T}$ as its standard matrix.
            \1 If $A$ is a matrix with linearly independent columns, then the pseudoinverse of $A$ is the matrix $A^{+}$defined by
            \[A^{+}=\left(A^{T} A\right)^{-1} A^{T}\]
            \1 Let $A$ be a matrix with linearly independent columns. Then the pseudoinverse $A^{+}$of $A$ satisfies the following properties, called the Penrose conditions for $A$:
                \2 $A A^{+} A=A$
                \2 $A^{+} A A^{+}=A^{+}$
                \2 $A A^{+}$and $A^{+} A$ are symmetric.
        \end{outline}
        \subsection{The Singular Value Decomposition} 
        \begin{outline}
            \1 If $A$ is an $m \times n$ matrix, the singular values of $A$ are the square roots of the eigenvalues of $A^{T} A$ and are denoted by $\sigma_{1}, \ldots, \sigma_{n}$. It is conventional to arrange the singular values so that $\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{n}$.
            \1 The Singular Value Decomposition: Let $A$ be an $m \times n$ matrix with singular values $\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r}>0$ and $\sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_{n}=0$. Then there exist an $m \times m$ orthogonal matrix $U$, an $n \times n$ orthogonal matrix $V$, and an $m \times n$ matrix $\Sigma$ of the form shown in Equation (1) such that
            \[A=U \Sigma V^{T}\]
            \1 The Outer Product Form of the SVD: Let $A$ be an $m \times n$ matrix with singular values $\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r}>0$ and $\sigma_{r+1}=$ $\sigma_{r+2}=\cdots=\sigma_{n}=0$. Let $\mathbf{u}_{1}, \ldots, \mathbf{u}_{r}$ be left singular vectors and let $\mathbf{v}_{1}, \ldots, \mathbf{v}_{r}$ be right singular vectors of $A$ corresponding to these singular values. Then
            \[A=\sigma_{1} \mathbf{u}_{1} \mathbf{v}_{1}^{T}+\cdots+\sigma_{r} \mathbf{u}_{r} \mathbf{v}_{r}^{T}\]
            \1 Let $A=U \Sigma V^{T}$ be a singular value decomposition of an $m \times n$ matrix $A$. Let $\sigma_{1}, \ldots$, $\sigma_{r}$ be all the nonzero singular values of $A$. Then:
                \2 The rank of $A$ is $r$.
                \2 $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{r}\right\}$ is an orthonormal basis for $\operatorname{col}(A)$.
                \2 $\left\{\mathbf{u}_{r+1}, \ldots, \mathbf{u}_{m}\right\}$ is an orthonormal basis for $\operatorname{null}\left(A^{T}\right)$.
                \2 $\left\{\mathbf{v}_{1}, \ldots, \mathbf{v}_{r}\right\}$ is an orthonormal basis for $\operatorname{row}(A)$.
                \2 $\left\{\mathbf{v}_{r+1}, \ldots, \mathbf{v}_{n}\right\}$ is an orthonormal basis for $\operatorname{null}(A)$.
            \1 Let $A$ be an $m \times n$ matrix with rank $r$. Then the image of the unit sphere in $\mathbb{R}^{n}$ under the matrix transformation that maps $\mathbf{x}$ to $A \mathbf{x}$ is
                \2 the surface of an ellipsoid in $\mathbb{R}^{m}$ if $r=n$.
                \2 a solid ellipsoid in $\mathbb{R}^{m}$ if $r<n$.
            \1 Let $A$ be an $m \times n$ matrix and let $\sigma_{1}, \ldots, \sigma_{r}$ be all the nonzero singular values of A. Then
            \[\|A\|_{F}=\sqrt{\sigma_{1}^{2}+\cdots+\sigma_{r}^{2}}\]
            \1 If $A$ is an $m \times n$ matrix and $Q$ is an $m \times m$ orthogonal matrix, then
            \[\|Q A\|_{F}=\|A\|_{F}\]
            \1 Let $A=U \Sigma V^{T}$ be an SVD for an $m \times n$ matrix $A$, where $\Sigma=$ $\left[\begin{array}{ll}D & O \\ O & O\end{array}\right]$ and $D$ is an $r \times r$ diagonal matrix containing the nonzero singular values $\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r}>0$ of $A$. The pseudoinverse (or Moore-Penrose inverse) of $A$ is the $n \times m$ matrix $A^{+}$defined by
            \[A^{+}=V \Sigma^{+} U^{T}\]
            where $\Sigma^{+}$is the $n \times m$ matrix \[\Sigma^{+}=\left[\begin{array}{cc}
            D^{-1} & O \\
            O & O
            \end{array}\right]\]
            \1 The least squares problem $A \mathbf{x}=\mathbf{b}$ has a unique least squares solution $\overline{\mathbf{x}}$ of minimal length that is given by \[\overline{\mathbf{x}}=A^{+} \mathbf{b}\]
            \1 The Fundamental Theorem of invertible matrices: Final Version. 
                \2 $A$ is invertible
                \2 $A\vb x=\vb b$ has a unique solution for every \(\vb b\) in \(\mathbb R^n\)
                \2 \(A\vb x=\vb 0\) has only the trivial solution. 
                \2 The reduced row echelon form of $A$ is \(I_n\). 
                \2 $A$ is the product of elementary matrices. 
                \2 \(\rank (A)=n\)
                \2 nullity\((A)=0\)
                \2 The column vectors of $A$ are linearly independent
                \2 The column vectors of $A$ span \(\mathbb R^n\)
                \2 The column vectors of $A$ form a basis for \(\mathbb R^n\)
                \2 The row vectors of $A$ are linearly independent
                \2 The row vectors of $A$ span \(\mathbb R^n\)
                \2 The row vectors of $A$ form a basis for \(\mathbb R^n\)
                \2 \(\det A\neq 0\)
                \2 $0$ is not an eigenvalue of $A$
                \2 $T$ is invertible. 
                \2 $T$ is one-to-one. 
                \2 $T$ is onto. 
                \2 \(\text{ker}(T)=\{\vb 0\}\)
                \2 \(\text{range}(T)=W\)
                \2 $0$ is not a singular value of $A$. 
    
        \end{outline}
        \subsection{Applications} 
        \begin{outline}
            \1 General problem of approximating functions can be stated as: Given a continuous function $f$ on an interval $[a, b]$ and a subspace $W$ of $\mathscr{C}[a, b]$, find the function "closest" to $f$ in $W$.
            \1 The n'th order Fourier approximation to $f$ on \([-\pi,\pi]\): \[\begin{aligned}
                &a_{0}=\frac{\langle 1, f\rangle}{\langle 1,1\rangle}=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) d x \\
                &a_{k}=\frac{\langle\cos k x, f\rangle}{\langle\cos k x, \cos k x\rangle}=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos k x d x \\
                &b_{k}=\frac{\langle\sin k x, f\rangle}{\langle\sin k x, \sin k x\rangle}=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin k x d x
            \end{aligned}\]
        \end{outline}
\end{document}
